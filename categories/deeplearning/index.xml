<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>DeepLearning on Toki 的个人博客</title><link>https://tok1024.com/categories/deeplearning/</link><description>Recent content in DeepLearning on Toki 的个人博客</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 30 Mar 2025 11:59:10 +0800</lastBuildDate><atom:link href="https://tok1024.com/categories/deeplearning/index.xml" rel="self" type="application/rss+xml"/><item><title>Bayesian Ai</title><link>https://tok1024.com/p/bayesian-ai/</link><pubDate>Sun, 30 Mar 2025 11:59:10 +0800</pubDate><guid>https://tok1024.com/p/bayesian-ai/</guid><description>&lt;h1 id="从贝叶斯视角理解-ai">从贝叶斯视角理解 AI
&lt;/h1>&lt;h2 id="引言-大模型写的作业会雷同吗">引言: 大模型写的作业会雷同吗?
&lt;/h2>&lt;p>你有没有想过这样一个问题: &lt;em>当你和室友一起把某门水课的大作业题目复制到大模型中时, 他会不会每次都产生完全一模一样的答案&lt;/em> ? 如果不会, 为什么呢?&lt;/p>
&lt;p>答案当然是不会。下面我将从概率论和贝叶斯视角出发，逐步解释为什么大模型不会生成完全相同的答案，以及为什么这种随机性实际上是一种优势。&lt;/p>
&lt;h2 id="条件概率与贝叶斯思想">条件概率与贝叶斯思想
&lt;/h2>&lt;h3 id="贝叶斯公式">贝叶斯公式
&lt;/h3>&lt;p>首先让我们回顾概率论中的贝叶斯公式:&lt;/p>
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$&lt;p>这个公式看似简单，却蕴含了贝叶斯学派的整个世界观。它不仅是一个计算条件概率的工具，更是一种思考和推理的方式。&lt;/p>
&lt;h3 id="先验与后验">先验与后验
&lt;/h3>&lt;p>贝叶斯学派认为: &lt;strong>概率不是客观频率，而是对不确定性的量化表达；学习是一个不断用新证据更新信念的过程&lt;/strong>。简言之，世界上没有绝对确定的事物，我们通过持续观察获取新证据，不断更新对世界的认知。&lt;/p>
&lt;p>从贝叶斯公式的角度理解，若将 $A$ 视为假设，$B$ 视为观测到的证据，则:&lt;/p>
&lt;ul>
&lt;li>$P(A)$ 是&lt;strong>先验概率&lt;/strong> (Prior)：在观测到证据 $B$ 之前，我们对假设 $A$ 成立概率的初始估计&lt;/li>
&lt;li>$P(A|B)$ 是&lt;strong>后验概率&lt;/strong> (Posterior)：在观测到证据 $B$ 之后，我们对假设 $A$ 成立概率的更新估计&lt;/li>
&lt;li>$P(B|A)$ 是&lt;strong>似然&lt;/strong> (Likelihood)：假设 $A$ 成立的条件下，观测到证据 $B$ 的概率&lt;/li>
&lt;li>$P(B)$ 是&lt;strong>边缘概率&lt;/strong> (Marginal Probability)：观测到证据 $B$ 的总体概率&lt;/li>
&lt;/ul>
&lt;p>用更直观的表达式:&lt;/p>
$$P(\text{假设}|\text{证据}) = \frac{P(\text{证据}|\text{假设}) \times P(\text{假设})}{P(\text{证据})}$$&lt;p>即: &lt;strong>后验 = 似然 × 先验 ÷ 证据概率&lt;/strong>&lt;/p>
&lt;h3 id="频率学派-vs-贝叶斯学派">频率学派 vs. 贝叶斯学派
&lt;/h3>&lt;p>贝叶斯学派与频率学派在概率解释和统计推断方面存在根本差异:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>频率学派&lt;/th>
&lt;th>贝叶斯学派&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>概率表示事件的长期频率&lt;/td>
&lt;td>概率表示信念的主观程度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>参数是固定但未知的常数&lt;/td>
&lt;td>参数是具有概率分布的随机变量&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>依赖 $p$ 值和置信区间&lt;/td>
&lt;td>使用后验分布和可信区间&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>不使用先验信息&lt;/td>
&lt;td>明确纳入先验信息&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>主要方法：最大似然估计 (MLE)&lt;/td>
&lt;td>主要方法：最大后验估计 (MAP)、完全后验分布&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="判别式模型">判别式模型
&lt;/h2>&lt;p>以图像分类为例，模型预测过程涉及三个主体：模型参数 $\theta$、输入数据 $\mathbf{x}$（及其真实标签 $y$）和预测结果 $\hat{y}$。从概率角度看，预测过程就是&lt;strong>在给定输入 $\mathbf{x}$ 的条件下，找出使条件概率 $P(y|\mathbf{x},\theta)$ 最大的类别 $\hat{y}$&lt;/strong>:&lt;/p>
$$\hat{y} = \arg\max_y P(y|\mathbf{x},\theta)$$&lt;p>关键问题是：如何确定最优的模型参数 $\theta$？这就涉及到不同的参数估计方法。&lt;/p>
&lt;h3 id="最大似然估计-mle">最大似然估计 (MLE)
&lt;/h3>&lt;p>频率学派采用最大似然估计，寻找能最大化观测数据概率的参数 (换句话说: 最能解释数据集的参数):&lt;/p>
$$\theta_{\text{MLE}} = \arg\max_\theta P(D|\theta)$$&lt;p>其中 $D = {(\mathbf{x}&lt;em>i, y_i)}&lt;/em>{i=1}^n$ 是训练数据集。假设数据独立同分布 (i.i.d.)，似然函数可表示为:&lt;/p>
$$P(D|\theta) = \prod_{i=1}^n P(y_i|\mathbf{x}_i,\theta)$$&lt;p>为便于计算，通常取对数转换乘积为求和:&lt;/p>
$$\log P(D|\theta) = \sum_{i=1}^n \log P(y_i|\mathbf{x}_i,\theta)$$&lt;p>这正是我们熟悉的交叉熵损失函数的负值。&lt;/p>
&lt;h3 id="最大后验估计-map">最大后验估计 (MAP)
&lt;/h3>&lt;p>贝叶斯学派则考虑参数 $\theta$ 的先验分布，采用最大后验估计:&lt;/p>
$$\theta_{\text{MAP}} = \arg\max_\theta P(\theta|D) = \arg\max_\theta \frac{P(D|\theta)P(\theta)}{P(D)}$$&lt;p>由于 $P(D)$ 对参数优化而言是常数，简化为:&lt;/p>
$$\theta_{\text{MAP}} = \arg\max_\theta P(D|\theta)P(\theta)$$&lt;p>取对数后:&lt;/p>
$$\theta_{\text{MAP}} = \arg\max_\theta \left[ \log P(D|\theta) + \log P(\theta) \right]$$&lt;p>可以看出，MAP 比 MLE 多了先验项 $\log P(\theta)$，这实际上起到了正则化作用。例如，当先验为高斯分布 $P(\theta) \sim \mathcal{N}(0, \sigma^2)$ 时，$\log P(\theta)$ 对应于 $L_2$ 正则化；当先验为拉普拉斯分布时，对应于 $L_1$ 正则化。&lt;/p>
&lt;h3 id="完全贝叶斯推断">完全贝叶斯推断
&lt;/h3>&lt;p>更进一步，完全贝叶斯推断不仅寻找单一最优参数点，而是&lt;strong>考虑所有可能参数及其概率分布&lt;/strong>:&lt;/p>
$$P(y|\mathbf{x}, D) = \int P(y|\mathbf{x}, \theta) P(\theta|D) d\theta$$&lt;p>这个积分通常难以解析计算，需要借助变分推断 (Variational Inference) 或马尔可夫链蒙特卡洛 (MCMC) 等近似方法。完全贝叶斯推断的优势在于能够量化预测的不确定性，而不仅仅给出点估计。&lt;/p>
&lt;h2 id="生成式模型">生成式模型
&lt;/h2>&lt;p>我们使用的大模型和上文的判别式模型有一个本质区别就是: &lt;strong>判别式模型给定输入 x 的情况下, 输出的 lable 是确定的&lt;/strong>, 但是大模型每次生成文本都有不同之处, 这是因为实际上每次大模型生成的是下一个词语的概率分布, 然后按照某种方式在其中采样, 我们把这种模型叫做&lt;strong>生成式模型&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-1.png"
width="1388"
height="733"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-1_hu_cc14b7d08b7e7759.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-1_hu_8b236c2fb61ef3dd.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-1"
class="gallery-image"
data-flex-grow="189"
data-flex-basis="454px"
>&lt;/p>
&lt;p>生成式模型特点&lt;/p>
&lt;ul>
&lt;li>一个输入多个输出：同一个输入可能对应多种合理的输出结果&lt;/li>
&lt;li>训练数据中没有确切的解：不像分类任务有唯一正确答案&lt;/li>
&lt;li>难以预测：输出空间通常非常大且复杂&lt;/li>
&lt;/ul>
&lt;p>生成式模型的核心目标是学习数据的分布，而不仅仅是将输入映射到特定输出。这使得它们能够生成新的、多样化的、符合真实数据分布的样本。&lt;/p>
&lt;h3 id="判别式-vs-生成式">判别式 vs 生成式
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-2.png"
width="1379"
height="789"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-2_hu_b0f975d65376da0f.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-2_hu_603ebab5390673c4.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-2"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="419px"
>&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-3.png"
width="1360"
height="746"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-3_hu_9ddcec929cac0dfc.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-3_hu_59a37c7c120a6365.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-3"
class="gallery-image"
data-flex-grow="182"
data-flex-basis="437px"
>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>判别式模型: $P(y|x)$ - 给定输入 x，预测标签 y 的概率&lt;/p>
&lt;ul>
&lt;li>例如：分类器、回归模型&lt;/li>
&lt;li>关注决策边界，区分不同类别&lt;/li>
&lt;li>通常计算效率更高，需要的数据更少&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>生成式模型: $P(x|y)$ 或 $P(x,y)$ 或 $P(x)$, 给定输入标签 y，生成一个最可能符合现实中数据分布的 x&lt;/p>
&lt;ul>
&lt;li>学习数据本身的联合分布&lt;/li>
&lt;li>能够生成新样本&lt;/li>
&lt;li>通常需要更多参数和训练数据&lt;/li>
&lt;li>提供更丰富的信息（联合概率分布）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="概率建模">概率建模
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-4.png"
width="1403"
height="745"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-4_hu_b844b71235ccf4e5.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-4_hu_2fc144657aa7cef5.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-4"
class="gallery-image"
data-flex-grow="188"
data-flex-basis="451px"
>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>潜在因子（Latent Factors）&lt;/strong>：用 z 表示隐藏变量，如物体的姿势（pose）、光照（lighting）、尺度（scale）等。这些因子本身服从简单分布。&lt;/p>
&lt;ul>
&lt;li>潜在空间通常是低维的，具有良好的结构&lt;/li>
&lt;li>潜在变量捕获了数据生成过程中的关键因素&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>观察值生成&lt;/strong>：观察数据 x（如图像）由 &amp;ldquo;世界模型&amp;rdquo; 渲染生成，而世界模型是关于 z 的函数。最终，观察值 x 会呈现复杂分布。&lt;/p>
&lt;ul>
&lt;li>世界模型可以看作是从简单分布到复杂分布的变换&lt;/li>
&lt;li>深度神经网络可以作为这种变换的强大近似器&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>我们通过深度学习对整个数据的分布进行建模, 使得可以采样出符合真实世界的数据 z, 而这个过程的关键就是概率&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>数学表示&lt;/strong>：&lt;/p>
&lt;p>&lt;strong>生成模型的核心思想是通过联合分布 P (x, z) 来求解数据的边缘分布 P (x)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>联合分布：$P(x,z) = P(x|z)P(z)$&lt;/li>
&lt;li>边缘分布：$P(x) = \int P(x|z)P(z)dz$&lt;/li>
&lt;li>后验分布：$P(z|x) = \frac{P(x|z)P(z)}{P(x)}$&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-5.png"
width="1361"
height="742"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-5_hu_8ba72b7b61144ef8.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-5_hu_4f821e14ea1a60d3.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-5"
class="gallery-image"
data-flex-grow="183"
data-flex-basis="440px"
>&lt;/p>
&lt;p>对估计的概率分布和数据的分布进行对比, 作为损失函数。常用的度量方式包括：&lt;/p>
&lt;ul>
&lt;li>KL 散度：$D_{KL}(P_{data}||P_{model})$&lt;/li>
&lt;li>最大似然估计：最大化 $\log P_{model}(x)$&lt;/li>
&lt;li>Wasserstein 距离：用于 WGAN 等模型&lt;/li>
&lt;/ul>
&lt;h2 id="deep-generative-models">Deep Generative Models
&lt;/h2>&lt;h3 id="表征学习">表征学习
&lt;/h3>&lt;p>深度学习的核心任务之一是表征学习, 即在原始数据中自动提取对任务有用的特征（即“表示”），而无需人工设计特征。&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-6.png"
width="1395"
height="779"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-6_hu_8ccbc38657ffc11c.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-6_hu_a5e2fd975a68e7af.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-6"
class="gallery-image"
data-flex-grow="179"
data-flex-basis="429px"
>&lt;/p>
&lt;p>为什么深度学习有用? 因为它的&lt;strong>分层结构&lt;/strong>可以自动学习数据的&lt;strong>层次化表示&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>低层特征：边缘、纹理、简单形状&lt;/li>
&lt;li>中层特征：部件、组合结构&lt;/li>
&lt;li>高层特征：语义概念、抽象表示&lt;/li>
&lt;/ul>
&lt;p>这种表示学习能力使深度学习模型能够捕获数据中的复杂模式。&lt;/p>
&lt;h3 id="建模概率分布">建模概率分布
&lt;/h3>&lt;p>既然深度学习可以通过表征学习去学习数据的信息, 那么他当然也可以学习数据的分布, 一种直观的的方式是, 我们通过模型的学习把简单分布建模为复杂分布&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-7.png"
width="1391"
height="715"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-7_hu_6413a9af038906de.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-7_hu_519c2e167c600b5c.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-7"
class="gallery-image"
data-flex-grow="194"
data-flex-basis="466px"
>&lt;/p>
&lt;ul>
&lt;li>使用神经网络参数化概率分布&lt;/li>
&lt;li>将简单的先验分布（如高斯分布）转换为复杂的数据分布&lt;/li>
&lt;li>学习数据的隐含结构和生成过程&lt;/li>
&lt;/ul>
&lt;p>POV:&lt;/p>
&lt;ul>
&lt;li>生成模型会将一些深度神经网络作为构建模块。&lt;/li>
&lt;li>就像深度神经网络会将某些 &amp;ldquo;层&amp;rdquo; 作为构建模块一样。&lt;/li>
&lt;li>生成模型是更高层级的抽象。&lt;/li>
&lt;/ul>
&lt;h3 id="使用方法">使用方法
&lt;/h3>&lt;p>生成式模型的核心是学习从条件信息 y 到目标数据 x 的映射过程：$P(x|y)$&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-8.png"
width="1340"
height="678"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-8_hu_91fcc7bd1df59c32.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-8_hu_b64014cac397bb3a.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-8"
class="gallery-image"
data-flex-grow="197"
data-flex-basis="474px"
>&lt;/p>
&lt;p>条件信息 y 在实际应用中可以是多种形式：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>文本描述&lt;/strong>：如&amp;quot;一只橙色的猫坐在窗台上&amp;quot;（文生图）&lt;/li>
&lt;li>&lt;strong>类别标签&lt;/strong>：如数字&amp;quot;7&amp;quot;（条件图像生成）&lt;/li>
&lt;li>&lt;strong>属性向量&lt;/strong>：如年龄、性别、表情等（人脸生成）&lt;/li>
&lt;li>&lt;strong>部分数据&lt;/strong>：如图像的一部分（图像补全）&lt;/li>
&lt;li>&lt;strong>其他模态数据&lt;/strong>：如音频、视频片段（跨模态生成）&lt;/li>
&lt;/ul>
&lt;p>本质上，y 是对生成空间的约束，它提供了低熵、高抽象的信息，而模型则负责将这些约束转化为高熵、高细节的具体数据 x。这种从抽象到具体的映射过程，使生成式模型能够在保持一致性的同时产生多样化的输出。&lt;/p>
&lt;p>深度生成模型的主要应用场景：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>数据生成&lt;/strong>：创建新的、逼真的样本&lt;/li>
&lt;li>&lt;strong>数据增强&lt;/strong>：为监督学习任务生成额外训练数据&lt;/li>
&lt;li>&lt;strong>异常检测&lt;/strong>：识别不符合学习分布的样本&lt;/li>
&lt;li>&lt;strong>缺失数据填补&lt;/strong>：根据部分观察推断完整数据&lt;/li>
&lt;li>&lt;strong>压缩表示&lt;/strong>：学习数据的紧凑编码&lt;/li>
&lt;/ol>
&lt;h2 id="主要生成模型类型">主要生成模型类型
&lt;/h2>&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-9.png"
width="1250"
height="740"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-9_hu_67503cf93b69e0c0.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-9_hu_137b3a870ad7c88f.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-9"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="405px"
>&lt;/p>
&lt;h3 id="自回归模型-autoregressive-models">自回归模型 (Autoregressive Models)
&lt;/h3>&lt;p>自回归模型将联合分布分解为条件概率的乘积：
$P(x) = \prod_{i=1}^{n} P(x_i|x_{&amp;lt;i})$&lt;/p>
&lt;p>特点：&lt;/p>
&lt;ul>
&lt;li>显式密度模型，可以直接计算似然&lt;/li>
&lt;li>生成过程是顺序的，每次生成一个元素&lt;/li>
&lt;li>代表模型：PixelRNN, PixelCNN, WaveNet, 语言模型&lt;/li>
&lt;li>最近特别强大的 GPT4o 就使用了自回归模型而非 diffusion model 来生成图像, 效果极其恐怖&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-10.jpg"
width="1024"
height="1536"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-10_hu_8f94989c5932e7f8.jpg 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-10_hu_d77d944712821c0.jpg 1024w"
loading="lazy"
alt="Bayesian-AI-image-10"
class="gallery-image"
data-flex-grow="66"
data-flex-basis="160px"
>&lt;/p>
&lt;h3 id="变分自编码器-vae">变分自编码器 (VAE)
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-11.png"
width="1558"
height="800"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-11_hu_18b6293ce7732e5.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-11_hu_1c08c2fb6df3acf1.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-11"
class="gallery-image"
data-flex-grow="194"
data-flex-basis="467px"
>&lt;/p>
&lt;p>VAE 通过变分推断学习潜在变量模型, 直接对一个图片建立一个概率分布：&lt;/p>
&lt;ul>
&lt;li>编码器网络：$q_\phi(z|x)$ 近似后验分布&lt;/li>
&lt;li>解码器网络：$p_\theta(x|z)$ 从潜在变量重建数据&lt;/li>
&lt;li>目标函数：ELBO (Evidence Lower BOund)
$$\mathcal{L}(\theta,\phi;x) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x)||p(z))$$&lt;/li>
&lt;/ul>
&lt;p>特点：&lt;/p>
&lt;ul>
&lt;li>学习有意义的潜在空间&lt;/li>
&lt;li>生成质量通常不如 GAN&lt;/li>
&lt;li>训练稳定，避免模式崩溃&lt;/li>
&lt;li>后续成为了 stable diffusion 的一个模块&lt;/li>
&lt;/ul>
&lt;h3 id="生成对抗网络-gan">生成对抗网络 (GAN)
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-12.png"
width="1528"
height="882"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-12_hu_fb0e9359eb1c0194.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-12_hu_18a94bc5e547e577.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-12"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>GAN 通过博弈论框架学习生成模型：&lt;/p>
&lt;ul>
&lt;li>生成器 G：创建看起来真实的样本&lt;/li>
&lt;li>判别器 D：区分真实样本和生成样本&lt;/li>
&lt;li>目标函数：&lt;/li>
&lt;/ul>
$$\min_G \max_D V(D,G) = \mathbb{E}_{x\sim p_{data}}[\log D(x)] + \mathbb{E}_{z\sim p_z}[\log(1-D(G(z)))]$$&lt;p>特点：&lt;/p>
&lt;ul>
&lt;li>生成质量极高&lt;/li>
&lt;li>训练不稳定，容易模式崩溃&lt;/li>
&lt;li>难以评估模型质量&lt;/li>
&lt;li>变种众多：DCGAN, WGAN, StyleGAN 等&lt;/li>
&lt;/ul>
&lt;p>效果图:&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-13.png"
width="636"
height="638"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-13_hu_fc381913958a240c.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-13_hu_74e1b8b55d27c21d.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-13"
class="gallery-image"
data-flex-grow="99"
data-flex-basis="239px"
> image 20250330101500. Png&lt;/p>
&lt;h3 id="扩散模型-diffusion-models">扩散模型 (Diffusion Models)
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-14.png"
width="1567"
height="877"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-14_hu_b1ace60cfff951ec.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-14_hu_36e5758cc264f73f.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-14"
class="gallery-image"
data-flex-grow="178"
data-flex-basis="428px"
>&lt;/p>
&lt;p>扩散模型通过逐步去噪学习生成过程：&lt;/p>
&lt;ul>
&lt;li>前向过程：逐步向数据添加噪声&lt;/li>
&lt;li>反向过程：学习去噪，恢复原始数据&lt;/li>
&lt;li>目标函数：预测添加的噪声&lt;/li>
&lt;/ul>
&lt;p>特点：&lt;/p>
&lt;ul>
&lt;li>生成质量超越 GAN&lt;/li>
&lt;li>训练稳定&lt;/li>
&lt;li>推理速度较慢&lt;/li>
&lt;li>代表模型：DDPM, DALL-E 2, Stable Diffusion&lt;/li>
&lt;/ul>
&lt;h2 id="总结">总结
&lt;/h2>&lt;p>本文通过贝叶斯视角揭示了现代AI的核心本质：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>贝叶斯思想的革命性影响&lt;/strong>：贝叶斯学派将概率从客观频率重新定义为不确定性的度量，这一转变是现代AI能够处理复杂、不确定世界的理论基础。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>判别式到生成式的范式转变&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>判别式模型（$P(y|x)$）只关注输入到输出的映射，本质上是确定性的&lt;/li>
&lt;li>生成式模型（$P(x,y)$或$P(x)$）学习数据本身的分布，能够产生多样化输出&lt;/li>
&lt;li>这一转变解释了为什么大模型能够对同一问题给出不同但合理的回答&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>潜变量与概率分布变换&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>引入潜变量$z$是现代生成模型的关键突破&lt;/li>
&lt;li>通过将复杂分布表示为简单分布的变换，AI获得了&amp;quot;创造性&amp;quot;&lt;/li>
&lt;li>深度神经网络作为这种变换的强大近似器，使复杂分布的建模成为可能&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>采样机制的重要性&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>大模型不是简单地记忆和重复，而是从学习到的概率分布中采样&lt;/li>
&lt;li>这种机制使AI能够生成新颖且多样的内容，而非固定输出&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>这种基于概率的视角不仅统一了从VAE到GAN再到扩散模型和自回归模型的技术路线，也为我们理解AI的能力边界提供了理论框架。正是贝叶斯思想的引入，使AI从简单的模式匹配工具进化为能够处理不确定性、具有创造性的智能系统。&lt;/p>
&lt;p>回到开篇的问题：大模型写的作业不会雷同，正是它们不是确定性地输出&amp;quot;最优解&amp;quot;，而是从学习到的概率分布中进行采样，这种基于概率的生成机制是现代AI最本质的特征。&lt;/p></description></item><item><title>My Transformer</title><link>https://tok1024.com/p/my-transformer/</link><pubDate>Sat, 15 Mar 2025 11:31:55 +0800</pubDate><guid>https://tok1024.com/p/my-transformer/</guid><description>&lt;h2 id="intro">Intro
&lt;/h2>&lt;p>最近在学习深度学习的基础知识, 对于五花八门的模型深感神奇, 大受震撼, 但是觉得实操能力欠佳, 于是尝试实操手搓一个 Transformer&lt;/p>
&lt;p>训练一个模型有四个步骤: 数据处理 -&amp;gt; 定义模型 -&amp;gt; 定义损失函数 -&amp;gt; 优化, 我们这次也将按照这个步骤进行, 过程参考 b 站视频 &lt;a class="link" href="https://www.bilibili.com/video/BV1BbFaeVE4W" target="_blank" rel="noopener"
>Pytorch手搓 Transformer&lt;/a>&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-1.png"
width="1501"
height="862"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-1_hu_1ec8aba735bd98d8.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-1_hu_1b230594cff6ccf6.png 1024w"
loading="lazy"
alt="My-Transformer-image-1"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="417px"
>&lt;/p>
&lt;h3 id="transformer">Transformer
&lt;/h3>&lt;p>首先, 什么是 transformer?&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-2.png"
width="537"
height="346"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-2_hu_a3c07671007e94c3.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-2_hu_81a9e1b46faacbdf.png 1024w"
loading="lazy"
alt="My-Transformer-image-2"
class="gallery-image"
data-flex-grow="155"
data-flex-basis="372px"
>&lt;/p>
&lt;p>当然不是变形金刚, Transformer 是一个基于 &lt;strong>Self-Attention&lt;/strong>机制的 &lt;strong>Seq2seq&lt;/strong> 的深度学习模型, 能够捕捉上下文的信息和序列数据, 可以并行训练, 现在已经得到广泛的应用, 我们熟悉的 BERT, GPT, Deepseek 都使用了 Transformer 架构, 足以证明其性能的优越性&lt;/p>
&lt;p>我们这次将训练一个非常简单的 transformer, 输入数据是上文, 设定一个生成的文本长度, 然后直接输出下文, 未来可能会把起始和结束标识编码到 embedding 向量中, 但我现在还不会&lt;/p>
&lt;h2 id="数据处理">数据处理
&lt;/h2>&lt;p>我们的原始数据是中文的文本文件, 要想存储到计算机中供模型训练, 就需要先把每个字转换为一个 &lt;code>token&lt;/code>, 再将 &lt;code>token&lt;/code> 经过 Embedding 嵌入为词向量&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-3.png"
width="1002"
height="525"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-3_hu_e542ce639874fbe2.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-3_hu_34e429d4d4e74803.png 1024w"
loading="lazy"
alt="My-Transformer-image-3"
class="gallery-image"
data-flex-grow="190"
data-flex-basis="458px"
>&lt;/p>
&lt;h3 id="token-化">Token 化
&lt;/h3>&lt;p>这一步中, 我们需要创建唯一, 有序的字符集, 然后建立数字即 Token 到字符的映射&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 有序的字符集合&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">chars&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">sorted&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">list&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">set&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 字符到数字的映射&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">c2i&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">i&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">chars&lt;/span>&lt;span class="p">)}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">i2c&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">c&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">enumerate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">chars&lt;/span>&lt;span class="p">)}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 编码: 字符串 -&amp;gt; 数字列表&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 解码: 数字列表 -&amp;gt; 字符串&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">encode&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="n">c2i&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">c&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">c&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">decode&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="s2">&amp;#34;&amp;#34;&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">join&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">i2c&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="数据分组">数据分组
&lt;/h3>&lt;p>训练模型时, 一条一条训练效率过于低下, 我们会选择一次处理一批数据, 这样可以利用 GPU 的并行性, 提高性能, 每一个向量是长度为 &lt;code>block_size&lt;/code> 的字符串&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-4.png"
width="1190"
height="708"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-4_hu_3b1257f6461f8a0d.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-4_hu_9bc4518e90917798.png 1024w"
loading="lazy"
alt="My-Transformer-image-4"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="403px"
>&lt;/p>
&lt;p>所以一批训练资料是 &lt;code>[batch_size, block_size, embedding_dim]&lt;/code> 的三阶张量&lt;/p>
&lt;h4 id="get_batch">&lt;code>get_batch&lt;/code>
&lt;/h4>&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-5.png"
width="1465"
height="442"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-5_hu_3c043398af04e503.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-5_hu_dfb321013003f8bd.png 1024w"
loading="lazy"
alt="My-Transformer-image-5"
class="gallery-image"
data-flex-grow="331"
data-flex-basis="795px"
>&lt;/p>
&lt;p>对于 batch 的选择, 我们随机在文本中取一段 block&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># TODO: 数据分批&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 1. 划分数据集&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 直接对text进行编码&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">encode&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dtype&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">long&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">valid_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">int&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">text&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">validation_split&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">valid_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">valid_size&lt;/span>&lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">train_data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">data&lt;/span>&lt;span class="p">[:&lt;/span>&lt;span class="n">valid_size&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 2. get_batch函数&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 从数据集中随机取出batch_size个数据&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 输入: split - &amp;#34;valid&amp;#34; or &amp;#34;train&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 输出: (batch_size, block_size)的tensor&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">get_batch&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">data&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">valid_data&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">split&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="s2">&amp;#34;valid&amp;#34;&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="n">train_data&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">idx&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">randint&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">len&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">block_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="n">batch_size&lt;/span>&lt;span class="p">,))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># stack处理一个列表,把一个张量的列表在新的维度上堆叠起来&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">block_size&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">idx&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">data&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">:&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="o">+&lt;/span>&lt;span class="n">block_size&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">idx&lt;/span>&lt;span class="p">])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># x是字符串的列表, y是x的下一个字符的列表&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">get_batch&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;train&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>这里用到了 &lt;code>torch.stack&lt;/code>, 让我想起来另一个常用的拼接 api &lt;code>torch.cat&lt;/code>, 二者有什么区别呢?&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 创建两个示例张量&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">a&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">2&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">3&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">4&lt;/span>&lt;span class="p">]])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">b&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tensor&lt;/span>&lt;span class="p">([[&lt;/span>&lt;span class="mi">5&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">6&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="mi">7&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">8&lt;/span>&lt;span class="p">]])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 使用 torch.cat 进行拼接&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">cat_result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;torch.cat 结果：&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">cat_result&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;形状：&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">cat_result&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 使用 torch.stack 进行拼接&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">stack_result&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">stack&lt;/span>&lt;span class="p">((&lt;/span>&lt;span class="n">a&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">b&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;&lt;/span>&lt;span class="se">\n&lt;/span>&lt;span class="s2">torch.stack 结果：&amp;#34;&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">stack_result&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nb">print&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;形状：&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">stack_result&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>输出结果:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">&lt;span class="line">&lt;span class="cl">torch.cat 结果：
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">tensor([[1, 2], [3, 4], [5, 6], [7, 8]])
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">形状： torch.Size([4, 2])
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">torch.stack 结果：
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">形状： torch.Size([2, 2, 2])
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>原来 stack 是在新的维度上连接原本的两个张量, 而 cat 是在外层维度拼接两个张量&lt;/p>
&lt;h3 id="词嵌入">词嵌入
&lt;/h3>&lt;p>我们已经有了字符到数字的映射, 但是现在这个数字没什么含义, 更无法参与运算, 那我们就需要把每个词表示为一个向量, 这就用到了 &lt;code>nn.Embedding&lt;/code> 子类, 创建一个 &lt;code>embedding_table&lt;/code>, 维护字符集的索引到 Embedding 空间的映射&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-6.png"
width="1514"
height="738"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-6_hu_e62424821608eb22.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-6_hu_59c890ba86b662d4.png 1024w"
loading="lazy"
alt="My-Transformer-image-6"
class="gallery-image"
data-flex-grow="205"
data-flex-basis="492px"
>&lt;/p>
&lt;p>一开始时, 嵌入向量随机生成, 然后不断梯度优化, 可能会和真实的语义有一定的相关性&lt;/p>
&lt;h3 id="位置嵌入">位置嵌入
&lt;/h3>&lt;p>我们知道, 在一个文本中, 词语和其在文本中的顺序是有很强的关系的, 这就需要把位置编码词向量, 这里我们同样用 pytorch 提供的 embedding 类进行嵌入, 与原文的正余弦不同, 后续可以进行调整&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-7.png"
width="1469"
height="631"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-7_hu_891943d336c3d25e.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-7_hu_4a31acff8e2c54df.png 1024w"
loading="lazy"
alt="My-Transformer-image-7"
class="gallery-image"
data-flex-grow="232"
data-flex-basis="558px"
>&lt;/p>
&lt;p>最后把词嵌入和位置嵌入的向量相加得到输入向量&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># 位置嵌入&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">position_embedding_table&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">block_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">position_ebd&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">position_embedding_table&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">block_size&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">))&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">unsqueeze&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">target&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">ve&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">pe&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">position_embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ve&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">pe&lt;/span> &lt;span class="c1"># (B, T, E)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">blocks&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ln_f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">logits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fc&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="模型构建">模型构建
&lt;/h2>&lt;h3 id="基础模型">基础模型
&lt;/h3>&lt;p>我们先考虑创建一个简单的傻瓜模型&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-8.png"
width="1489"
height="693"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-8_hu_7e9314b5a868c7e5.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-8_hu_c3fe7e89ead71e13.png 1024w"
loading="lazy"
alt="My-Transformer-image-8"
class="gallery-image"
data-flex-grow="214"
data-flex-basis="515px"
>&lt;/p>
&lt;p>把嵌入后的张量输入神经网络, 他会随机输出一个张量, 维度是 &lt;code>[Batch_size, Block_size, Vocabsize]&lt;/code>, 其中最后一维是归一化的, 代表了下文中每个 Token 的概率, 不过这里是随机生成的&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-9.png"
width="847"
height="424"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-9_hu_c0043729fb7c4895.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-9_hu_f569e8e835b00b9d.png 1024w"
loading="lazy"
alt="My-Transformer-image-9"
class="gallery-image"
data-flex-grow="199"
data-flex-basis="479px"
>&lt;/p>
&lt;p>Token 生成: 从概率分布中抽样出 one-hot 向量, 使用 &lt;code>torch.multinomial&lt;/code>&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-10.png"
width="1069"
height="381"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-10_hu_39c77b0f500511a5.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-10_hu_67cf027f15c0d3de.png 1024w"
loading="lazy"
alt="My-Transformer-image-10"
class="gallery-image"
data-flex-grow="280"
data-flex-basis="673px"
>&lt;/p>
&lt;p>实现:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Model&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">LanguageModel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># x: (batch_size, block_size) 单位是token&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># target: (batch_size, block_size) 单位是token&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 返回: (batch_size, block_size, vocab_size) logits&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">target&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">random_tensor&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">rand&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vocab_size&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">random_tensor&lt;/span> &lt;span class="o">/&lt;/span> &lt;span class="n">random_tensor&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sum&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">keepdim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">True&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">logits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 生成&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># token_seq: (batch_size, block_size) 上文, 单位是token&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># max_token: int 最大生成长度&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 返回: (batch_size, max_token) 生成的token序列&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">generate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">token_seq&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_token&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">max_token&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 取最后block_size个token&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">token_input&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">token_seq&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">block_size&lt;/span>&lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 计算logits&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">token_input&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 取字符串的最后一个字符, 目前还只是网络直接输出的结果&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">logits&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># softmax,维度是-1,也就是vocabulary的维度&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">prob&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">F&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">logits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 采样, 输出是下一个token,形状是(batch_size, 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">next_token&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">multinomial&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prob&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 拼接到token_seq后面, 在时间维度上&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">token_seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">token_seq&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">next_token&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">token_seq&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">max_token&lt;/span>&lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="矩阵变换">矩阵变换
&lt;/h3>&lt;p>在神经网络中, 我们不会用嵌入向量来进行计算, 而是把词向量&lt;strong>投影到不同的子空间中&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Q: 查询矩阵, 定义了该 Token 如何去访问别的 Token 的信息&lt;/li>
&lt;li>K: 键矩阵, 定义了该 Token 给别的矩阵提供哪些信息&lt;/li>
&lt;li>V: 值矩阵, 定义了词向量到我们创建的子空间的映射&lt;/li>
&lt;/ul>
&lt;p>再看下论文中优美的公式&lt;/p>
&lt;p>$Attention(Q,K,V)=softmax(\frac{ QK^T} {d_k}​)V$&lt;/p>
&lt;p>几何意义：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>点积&lt;/strong>：在子空间 Rdk​ 中，计算两个向量的夹角余弦相似度（未缩放时）。&lt;/li>
&lt;li>&lt;strong>缩放因子 dk​​&lt;/strong>：防止点积值过大导致梯度消失。&lt;/li>
&lt;li>&lt;strong>softmax&lt;/strong>：将相似度转化为概率分布，表示不同位置的重要性权重。&lt;/li>
&lt;/ul>
&lt;p>这个公式精准的描述了 Attention 机制, 即用 Q 去查询 K, 对得到的矩阵除以一个缩放因子(防止梯度爆炸), 输入 softmax 得到注意力矩阵, 然后和 V 矩阵相乘后, 得到了 Token 的概率分布&lt;/p>
&lt;h3 id="掩码矩阵">掩码矩阵
&lt;/h3>&lt;p>实践中, 注意力矩阵不能全部都有值, 因为一个预测模型不能输入未来的向量, 这样会破坏模型结构&lt;/p>
&lt;p>我们用下三角矩阵来表示&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-11.png"
width="1476"
height="695"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-11_hu_63b00df0cc7c60f8.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-11_hu_c807952f50e59537.png 1024w"
loading="lazy"
alt="My-Transformer-image-11"
class="gallery-image"
data-flex-grow="212"
data-flex-basis="509px"
>&lt;/p>
&lt;p>每一个值预测时, 我们只看上文, 防止答案泄露&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-12.png"
width="1455"
height="663"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-12_hu_1e4865f1fc10aad8.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-12_hu_fa563062908c6a04.png 1024w"
loading="lazy"
alt="My-Transformer-image-12"
class="gallery-image"
data-flex-grow="219"
data-flex-basis="526px"
>&lt;/p>
&lt;p>不可训练的矩阵: 三角矩阵, Tril 把右上角取为 0&lt;/p>
&lt;p>单头注意力的实现&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Heads&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Head&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">    &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">head_size&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">value&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">key&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">head_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">bias&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">False&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="c1"># 生成一个不可训练的下三角矩阵&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">register_buffer&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s2">&amp;#34;mask&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">tril&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ones&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">block_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">block_size&lt;/span>&lt;span class="p">)))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dropout&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Dropout&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dropout_rate&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">    &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="c1"># x: (batch_size, block_size, embedding_dim)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="c1"># return: (batch_size, block_size, head_size)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="c1"># 每个head有一个value矩阵, 用于计算attention&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">C&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">Q&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">query&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">K&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">key&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">attention&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">Q&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">K&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">transpose&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">C&lt;/span> &lt;span class="o">**&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mf">0.5&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">attention&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attention&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">masked_fill&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mask&lt;/span> &lt;span class="o">==&lt;/span> &lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="nb">float&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="s1">&amp;#39;-inf&amp;#39;&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="c1"># 输出的结果是 value向量 * attention&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">V&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">value&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">attention&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">F&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">attention&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">attention&lt;/span> &lt;span class="o">@&lt;/span> &lt;span class="n">V&lt;/span> &lt;span class="c1"># (B, T, head_size)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="k">return&lt;/span>  &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dropout&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">out&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="多头注意力">多头注意力
&lt;/h3>&lt;p>Attention 机制实际是在模仿人类阅读和写作时的注意力, 那么人都可以三心二意, 机器为什么不行 (&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-13.png"
width="1456"
height="709"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-13_hu_9541afb8de28e8d8.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-13_hu_4650762ff76e2469.png 1024w"
loading="lazy"
alt="My-Transformer-image-13"
class="gallery-image"
data-flex-grow="205"
data-flex-basis="492px"
>&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-14.png"
width="1189"
height="625"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-14_hu_9eb992b8c448ffb4.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-14_hu_86d4710e6ec004e.png 1024w"
loading="lazy"
alt="My-Transformer-image-14"
class="gallery-image"
data-flex-grow="190"
data-flex-basis="456px"
>&lt;/p>
&lt;p>所以我们把 embedding 分成多份, 分别用多个注意力头去关注整个向量&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-15.png"
width="1063"
height="442"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-15_hu_49ae9e10b62aacb9.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-15_hu_61db52740bcf90a.png 1024w"
loading="lazy"
alt="My-Transformer-image-15"
class="gallery-image"
data-flex-grow="240"
data-flex-basis="577px"
>&lt;/p>
&lt;p>实现:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">MultiHead&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">    &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">heads&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ModuleList&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">Head&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">head_num&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">head_num&lt;/span> &lt;span class="o">*&lt;/span> &lt;span class="n">head_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dropout&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Dropout&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dropout_rate&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">    &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="c1"># x: (batch_size, block_size, embedding_dim)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="c1"># return: (batch_size, block_size, embedding_dim)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fc&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">h&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">h&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">heads&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dropout&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">out&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="k">return&lt;/span> &lt;span class="n">out&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h3 id="残差连接">残差连接
&lt;/h3>&lt;p>实践中, 加入残差连接和 Layer Norm 效果会更好&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-16.png"
width="1448"
height="1087"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-16_hu_f3168462dedc1dda.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-16_hu_8c7661ffb333db24.png 1024w"
loading="lazy"
alt="My-Transformer-image-16"
class="gallery-image"
data-flex-grow="133"
data-flex-basis="319px"
>&lt;/p>
&lt;h4 id="residual-connection">Residual connection:
&lt;/h4>&lt;p>也就是把输出的 b 向量加上输入的 a 向量, 一个理解是我们 QKV 矩阵变换实际上计算出的向量, 可以理解为一个词语向量在上下文中的偏移, 要加上原本的向量才更加稳定, 不管怎么样, 他 works, 可以缓解梯度消失问题&lt;/p>
&lt;h4 id="layer-norm">Layer Norm:
&lt;/h4>&lt;p>区分于 BatchNorm, BN 是对整个 batch 的同一个 dimension 的 feature 进行归一化, LN 是对同一个向量的不同 dimension 归一化&lt;/p>
&lt;p>我们不仅 self-attention 的输出要残差连接和归一化, 输入进 FC 的也要进行残差连接和归一化, 于是直接把这个整体封装成一个 Block&lt;/p>
&lt;h4 id="block">Block
&lt;/h4>&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">Block&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">    &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ln1&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ln2&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sa&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">MultiHead&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ff&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Sequential&lt;/span>&lt;span class="p">(&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">            &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">hidden_dim&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">            &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ReLU&lt;/span>&lt;span class="p">(),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">            &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Dropout&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dropout_rate&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">            &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">hidden_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">),&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">            &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Dropout&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">dropout_rate&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">    &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="c1"># x: (batch_size, block_size, embedding_dim)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="c1"># return: (batch_size, block_size, embedding_dim)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">sa&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ln1&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ln2&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">out&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ff&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">out&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">        &lt;span class="k">return&lt;/span> &lt;span class="n">out&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>值得一提的是, 这里原论文是先输进注意力层, 再 ln, 叫做 post-ln, 有论文说 pre-ln 效果更好, 我也采用了 pre-ln, 不过感觉没啥提升, 可能是我数据太烂了, 没怎么做清洗, 这不是重点&lt;/p>
&lt;h3 id="多级残差网络">多级残差网络
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-17.png"
width="1390"
height="552"
srcset="https://tok1024.com/p/my-transformer/images/My%20Transformer-image-17_hu_8bc62412eda5174e.png 480w, https://tok1024.com/p/my-transformer/images/My%20Transformer-image-17_hu_c6d4988e403b212.png 1024w"
loading="lazy"
alt="My-Transformer-image-17"
class="gallery-image"
data-flex-grow="251"
data-flex-basis="604px"
>&lt;/p>
&lt;p>为了增加模型的复杂性, 我们会连接多个 block, 形成复杂的网络, 在 pytorch 中也很好实现这一点, 于是模型最终版完成了&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;span class="lnt">38
&lt;/span>&lt;span class="lnt">39
&lt;/span>&lt;span class="lnt">40
&lt;/span>&lt;span class="lnt">41
&lt;/span>&lt;span class="lnt">42
&lt;/span>&lt;span class="lnt">43
&lt;/span>&lt;span class="lnt">44
&lt;/span>&lt;span class="lnt">45
&lt;/span>&lt;span class="lnt">46
&lt;/span>&lt;span class="lnt">47
&lt;/span>&lt;span class="lnt">48
&lt;/span>&lt;span class="lnt">49
&lt;/span>&lt;span class="lnt">50
&lt;/span>&lt;span class="lnt">51
&lt;/span>&lt;span class="lnt">52
&lt;/span>&lt;span class="lnt">53
&lt;/span>&lt;span class="lnt">54
&lt;/span>&lt;span class="lnt">55
&lt;/span>&lt;span class="lnt">56
&lt;/span>&lt;span class="lnt">57
&lt;/span>&lt;span class="lnt">58
&lt;/span>&lt;span class="lnt">59
&lt;/span>&lt;span class="lnt">60
&lt;/span>&lt;span class="lnt">61
&lt;/span>&lt;span class="lnt">62
&lt;/span>&lt;span class="lnt">63
&lt;/span>&lt;span class="lnt">64
&lt;/span>&lt;span class="lnt">65
&lt;/span>&lt;span class="lnt">66
&lt;/span>&lt;span class="lnt">67
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Model&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">class&lt;/span> &lt;span class="nc">LanguageModel&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Module&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="fm">__init__&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="nb">super&lt;/span>&lt;span class="p">()&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="fm">__init__&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_embedding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">vocab_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">position_embedding&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">block_size&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">blocks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Sequential&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">Block&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_blocks&lt;/span>&lt;span class="p">)])&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">dropout&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Dropout&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mf">0.2&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ln_f&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">LayerNorm&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fc&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">nn&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">Linear&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">embedding_dim&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">vocab_size&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># x: (batch_size, block_size) 单位是token&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># target: (batch_size, block_size) 单位是token&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 返回: (batch_size, block_size, vocab_size) logits&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">target&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="kc">None&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">B&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">T&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">shape&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">ve&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vocab_embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">pe&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">position_embedding&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">arange&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">to&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">device&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">ve&lt;/span> &lt;span class="o">+&lt;/span> &lt;span class="n">pe&lt;/span> &lt;span class="c1"># (B, T, E)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">blocks&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">h&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">ln_f&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">fc&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">h&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 计算loss&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">if&lt;/span> &lt;span class="n">target&lt;/span> &lt;span class="ow">is&lt;/span> &lt;span class="ow">not&lt;/span> &lt;span class="kc">None&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">F&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cross_entropy&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">logits&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">B&lt;/span>&lt;span class="o">*&lt;/span>&lt;span class="n">T&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">),&lt;/span> &lt;span class="n">target&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">view&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">))&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">else&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="kc">None&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">logits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 生成&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># token_seq: (batch_size, block_size) 上文, 单位是token&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># max_token: int 最大生成长度&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 返回: (batch_size, max_token) 生成的token序列&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">def&lt;/span> &lt;span class="nf">generate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="bp">self&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">token_seq&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">max_token&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">_&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">max_token&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 取最后block_size个token&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">token_input&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">token_seq&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">block_size&lt;/span>&lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 计算logits&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="bp">self&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">forward&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">token_input&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 取字符串的最后一个字符, 目前还只是网络直接输出的结果&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">logits&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># softmax,维度是-1,也就是vocabulary的维度&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">prob&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">F&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">softmax&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">logits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=-&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 采样, 输出是下一个token,形状是(batch_size, 1)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">next_token&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">multinomial&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">prob&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="c1"># 拼接到token_seq后面, 在时间维度上&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">token_seq&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cat&lt;/span>&lt;span class="p">([&lt;/span>&lt;span class="n">token_seq&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">next_token&lt;/span>&lt;span class="p">],&lt;/span> &lt;span class="n">dim&lt;/span>&lt;span class="o">=&lt;/span>&lt;span class="mi">1&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">token_seq&lt;/span>&lt;span class="p">[:,&lt;/span> &lt;span class="o">-&lt;/span>&lt;span class="n">max_token&lt;/span>&lt;span class="p">:]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="nd">@torch.no_grad&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">estimate&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">model&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">splits&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">[&lt;/span>&lt;span class="s2">&amp;#34;train&amp;#34;&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="s2">&amp;#34;valid&amp;#34;&lt;/span>&lt;span class="p">]&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">eval&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="p">{}&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">split&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="n">splits&lt;/span>&lt;span class="p">:&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">losses&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">zeros&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_interval&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">for&lt;/span> &lt;span class="n">i&lt;/span> &lt;span class="ow">in&lt;/span> &lt;span class="nb">range&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_interval&lt;/span>&lt;span class="p">):&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">get_batch&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">logits&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">loss&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">model&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="n">y&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">losses&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">i&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">loss&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">item&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">out&lt;/span>&lt;span class="p">[&lt;/span>&lt;span class="n">split&lt;/span>&lt;span class="p">]&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">losses&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">model&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">train&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">out&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;h2 id="复盘">复盘
&lt;/h2>&lt;h3 id="成果">成果
&lt;/h3>&lt;p>用一些名著训练看看效果吧, 首先调整一下超参数&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="c1"># Hyperparameters&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">random_seed&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">3221&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">manual_seed&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">random_seed&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">batch_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">128&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">block_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">256&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_blocks&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">4&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">head_num&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">12&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">embedding_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">192&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">validation_split&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.2&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">device&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="s2">&amp;#34;cuda&amp;#34;&lt;/span> &lt;span class="k">if&lt;/span> &lt;span class="n">torch&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">cuda&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">is_available&lt;/span>&lt;span class="p">()&lt;/span> &lt;span class="k">else&lt;/span> &lt;span class="s2">&amp;#34;cpu&amp;#34;&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">wrapped_width&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">50&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">hidden_dim&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">768&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_epochs&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">1000&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">learning_rate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">5&lt;/span>&lt;span class="n">e&lt;/span>&lt;span class="o">-&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">weight_decay&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.06&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">patience&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mi">100&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">dropout_rate&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="mf">0.1&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">num_interval&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="nb">max&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">num_epochs&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="mi">10&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">50&lt;/span>&lt;span class="p">)&lt;/span>  &lt;span class="c1"># 每5%的epochs或至少每10个epochs验证一次&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="n">head_size&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">embedding_dim&lt;/span> &lt;span class="o">//&lt;/span> &lt;span class="n">head_num&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>训练过程耗时 11m 21.8s&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;span class="lnt">19
&lt;/span>&lt;span class="lnt">20
&lt;/span>&lt;span class="lnt">21
&lt;/span>&lt;span class="lnt">22
&lt;/span>&lt;span class="lnt">23
&lt;/span>&lt;span class="lnt">24
&lt;/span>&lt;span class="lnt">25
&lt;/span>&lt;span class="lnt">26
&lt;/span>&lt;span class="lnt">27
&lt;/span>&lt;span class="lnt">28
&lt;/span>&lt;span class="lnt">29
&lt;/span>&lt;span class="lnt">30
&lt;/span>&lt;span class="lnt">31
&lt;/span>&lt;span class="lnt">32
&lt;/span>&lt;span class="lnt">33
&lt;/span>&lt;span class="lnt">34
&lt;/span>&lt;span class="lnt">35
&lt;/span>&lt;span class="lnt">36
&lt;/span>&lt;span class="lnt">37
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">&lt;span class="line">&lt;span class="cl">Epoch 0, Loss: 8.412135124206543
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Train Loss: 8.115385055541992
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Valid Loss: 8.124889373779297
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--------------------------------------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Epoch 100, Loss: 5.311680793762207
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Train Loss: 5.232851982116699
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Valid Loss: 5.768039703369141
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--------------------------------------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Epoch 200, Loss: 4.538897514343262
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Train Loss: 4.509884357452393
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Valid Loss: 5.323651313781738
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--------------------------------------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Epoch 300, Loss: 4.282341480255127
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Train Loss: 4.204404354095459
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Valid Loss: 5.213500499725342
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--------------------------------------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Epoch 400, Loss: 4.078436851501465
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Train Loss: 4.0135650634765625
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Valid Loss: 5.163753032684326
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--------------------------------------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Epoch 500, Loss: 3.9056577682495117
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Train Loss: 3.8425979614257812
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Valid Loss: 5.133504867553711
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--------------------------------------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Epoch 600, Loss: 3.766578435897827
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Train Loss: 3.689257860183716
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Valid Loss: 5.1122727394104
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--------------------------------------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Epoch 700, Loss: 3.659522294998169
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Train Loss: 3.543461799621582
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Valid Loss: 5.107848644256592
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--------------------------------------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Epoch 800, Loss: 3.543654203414917
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Train Loss: 3.4007351398468018
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Valid Loss: 5.122027397155762
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--------------------------------------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">Early stopping!
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>生成点文字看看:&lt;/p>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt"> 1
&lt;/span>&lt;span class="lnt"> 2
&lt;/span>&lt;span class="lnt"> 3
&lt;/span>&lt;span class="lnt"> 4
&lt;/span>&lt;span class="lnt"> 5
&lt;/span>&lt;span class="lnt"> 6
&lt;/span>&lt;span class="lnt"> 7
&lt;/span>&lt;span class="lnt"> 8
&lt;/span>&lt;span class="lnt"> 9
&lt;/span>&lt;span class="lnt">10
&lt;/span>&lt;span class="lnt">11
&lt;/span>&lt;span class="lnt">12
&lt;/span>&lt;span class="lnt">13
&lt;/span>&lt;span class="lnt">14
&lt;/span>&lt;span class="lnt">15
&lt;/span>&lt;span class="lnt">16
&lt;/span>&lt;span class="lnt">17
&lt;/span>&lt;span class="lnt">18
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-plaintext" data-lang="plaintext">&lt;span class="line">&lt;span class="cl">--------------------------------------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">上文:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">余。 同时，总的生产规模之扩大，当然增加那种不是直接有赖于个别企业大小的经济。这些经济中最重要的，是由于相关的工业部门的发达而产生的，这些部门互相帮助，也许集
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">中在同一地方，但无论如何，它们都利用轮船、火车、电报、印刷机等所提供的近代交通便利。像这种来源所产生的各种经济，是任何生产部门都可获得的，而不是完全依靠它自己的
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">发达：但是，这些经济必然是随着它自己的发达而迅速地和稳步地增大；如果它衰败的话，这些经济在某些方面—— 虽然不是在一切方面——必然是缩小的。
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">第二节　生产费用应当以一个代表性企业来说明，这
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--------------------------------------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">真实下文:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">个企业能正常地获得属于一定的总生产量的内部经济与外部经济。报酬不变与报酬递增。 当我们研究支配一种商品的供给价格之各种原因时，这些结果具有很大的重要性。我们必
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">须仔细分析生产一种商品与一定的总生产量有关的正常费用；为了这个目的，我们将要研究在那个总生产量之下一个代表性生产者的费用。一方面，我们不要选择某一刚刚竭力投身营
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">业的新生产者为代表，他在许多不利的条件下经营，一时不得不满足于很少的利润或没有利润，但他对以下的事实是满意的；他正在建立营业关系，对于建立成功的营业正有头绪；另
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">一方面，我们也不要采取这样一个企业为代表：由于非常持久的能力和好运气，它已经有了很大的营业和井井有条的大工场，而这些大工
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">--------------------------------------------------
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">生成下文:
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">些参与备有收入和公司机的关系。雷益似存在深认为，我付不要大多用这一个新加上升的经济信息，它可以先衰退出于他们所作用加就适了此，附高的例外里学习的时就是因素解它。
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">我的行为使用于学说：无能支付的研究别人地者和银到这家愿意识良好工业，获得将会在因非营销反悔的技能性组织、不同的冒险的一种，看待着较有良好，当地工作更多的陈产也也
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">就产项目希望财富为是工员。真正越多的和同样的每一种情习这种成为，而是世界上受过的生的心理解它的机会，经纪人就增加了。正式和政府的“后，你想将来看承虚拟时间的梦想
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">忘镜子。”超过去在太富了贫穷人不变化，还到10美元的“变成虚拟轻松工作”中，而自己也是然寻法律的这一个人所组成了，但现恶
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>&lt;p>可以看出来效果还是不错的, 虽然没什么语义, 但是标点符号基本都能对上, 看着也像个句子, 嗯, 很满意&lt;/p>
&lt;h3 id="问题分析">问题分析
&lt;/h3>&lt;ul>
&lt;li>可以看到训练过程中出现了明显的过拟合问题, 应该主要是数据不足的问题&lt;/li>
&lt;li>训练过程中, 验证集的 loss 计算的很慢, 跟训练的时间都差不多了, 这个后续可以优化一下&lt;/li>
&lt;li>超参数和模型没有太多优化, 因为模型训练太慢了, 我也懒得等&amp;hellip;&lt;/li>
&lt;/ul>
&lt;h3 id="总结">总结
&lt;/h3>&lt;p>很好玩的一次实践, 之前一直对 pytorch 里张量的维数有点晕, 实操一次下来就比较清晰了, 对 transformer 的认识也更加清晰了, 非常感谢 b 站 up &lt;a class="link" href="https://space.bilibili.com/1570063857" target="_blank" rel="noopener"
>黯淡蓝点的居民&lt;/a>的视频和 NTU 李宏毅老师的机器学习课程&lt;/p></description></item></channel></rss>