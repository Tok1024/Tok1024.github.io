<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Math on Toki 的个人博客</title><link>https://tok1024.com/categories/math/</link><description>Recent content in Math on Toki 的个人博客</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Sun, 30 Mar 2025 11:59:10 +0800</lastBuildDate><atom:link href="https://tok1024.com/categories/math/index.xml" rel="self" type="application/rss+xml"/><item><title>Bayesian Ai</title><link>https://tok1024.com/p/bayesian-ai/</link><pubDate>Sun, 30 Mar 2025 11:59:10 +0800</pubDate><guid>https://tok1024.com/p/bayesian-ai/</guid><description>&lt;h1 id="从贝叶斯视角理解-ai">从贝叶斯视角理解 AI
&lt;/h1>&lt;h2 id="引言-大模型写的作业会雷同吗">引言: 大模型写的作业会雷同吗?
&lt;/h2>&lt;p>你有没有想过这样一个问题: &lt;em>当你和室友一起把某门水课的大作业题目复制到大模型中时, 他会不会每次都产生完全一模一样的答案&lt;/em> ? 如果不会, 为什么呢?&lt;/p>
&lt;p>答案当然是不会。下面我将从概率论和贝叶斯视角出发，逐步解释为什么大模型不会生成完全相同的答案，以及为什么这种随机性实际上是一种优势。&lt;/p>
&lt;h2 id="条件概率与贝叶斯思想">条件概率与贝叶斯思想
&lt;/h2>&lt;h3 id="贝叶斯公式">贝叶斯公式
&lt;/h3>&lt;p>首先让我们回顾概率论中的贝叶斯公式:&lt;/p>
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$&lt;p>这个公式看似简单，却蕴含了贝叶斯学派的整个世界观。它不仅是一个计算条件概率的工具，更是一种思考和推理的方式。&lt;/p>
&lt;h3 id="先验与后验">先验与后验
&lt;/h3>&lt;p>贝叶斯学派认为: &lt;strong>概率不是客观频率，而是对不确定性的量化表达；学习是一个不断用新证据更新信念的过程&lt;/strong>。简言之，世界上没有绝对确定的事物，我们通过持续观察获取新证据，不断更新对世界的认知。&lt;/p>
&lt;p>从贝叶斯公式的角度理解，若将 $A$ 视为假设，$B$ 视为观测到的证据，则:&lt;/p>
&lt;ul>
&lt;li>$P(A)$ 是&lt;strong>先验概率&lt;/strong> (Prior)：在观测到证据 $B$ 之前，我们对假设 $A$ 成立概率的初始估计&lt;/li>
&lt;li>$P(A|B)$ 是&lt;strong>后验概率&lt;/strong> (Posterior)：在观测到证据 $B$ 之后，我们对假设 $A$ 成立概率的更新估计&lt;/li>
&lt;li>$P(B|A)$ 是&lt;strong>似然&lt;/strong> (Likelihood)：假设 $A$ 成立的条件下，观测到证据 $B$ 的概率&lt;/li>
&lt;li>$P(B)$ 是&lt;strong>边缘概率&lt;/strong> (Marginal Probability)：观测到证据 $B$ 的总体概率&lt;/li>
&lt;/ul>
&lt;p>用更直观的表达式:&lt;/p>
$$P(\text{假设}|\text{证据}) = \frac{P(\text{证据}|\text{假设}) \times P(\text{假设})}{P(\text{证据})}$$&lt;p>即: &lt;strong>后验 = 似然 × 先验 ÷ 证据概率&lt;/strong>&lt;/p>
&lt;h3 id="频率学派-vs-贝叶斯学派">频率学派 vs. 贝叶斯学派
&lt;/h3>&lt;p>贝叶斯学派与频率学派在概率解释和统计推断方面存在根本差异:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>频率学派&lt;/th>
&lt;th>贝叶斯学派&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>概率表示事件的长期频率&lt;/td>
&lt;td>概率表示信念的主观程度&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>参数是固定但未知的常数&lt;/td>
&lt;td>参数是具有概率分布的随机变量&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>依赖 $p$ 值和置信区间&lt;/td>
&lt;td>使用后验分布和可信区间&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>不使用先验信息&lt;/td>
&lt;td>明确纳入先验信息&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>主要方法：最大似然估计 (MLE)&lt;/td>
&lt;td>主要方法：最大后验估计 (MAP)、完全后验分布&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="判别式模型">判别式模型
&lt;/h2>&lt;p>以图像分类为例，模型预测过程涉及三个主体：模型参数 $\theta$、输入数据 $\mathbf{x}$（及其真实标签 $y$）和预测结果 $\hat{y}$。从概率角度看，预测过程就是&lt;strong>在给定输入 $\mathbf{x}$ 的条件下，找出使条件概率 $P(y|\mathbf{x},\theta)$ 最大的类别 $\hat{y}$&lt;/strong>:&lt;/p>
$$\hat{y} = \arg\max_y P(y|\mathbf{x},\theta)$$&lt;p>关键问题是：如何确定最优的模型参数 $\theta$？这就涉及到不同的参数估计方法。&lt;/p>
&lt;h3 id="最大似然估计-mle">最大似然估计 (MLE)
&lt;/h3>&lt;p>频率学派采用最大似然估计，寻找能最大化观测数据概率的参数 (换句话说: 最能解释数据集的参数):&lt;/p>
$$\theta_{\text{MLE}} = \arg\max_\theta P(D|\theta)$$&lt;p>其中 $D = {(\mathbf{x}&lt;em>i, y_i)}&lt;/em>{i=1}^n$ 是训练数据集。假设数据独立同分布 (i.i.d.)，似然函数可表示为:&lt;/p>
$$P(D|\theta) = \prod_{i=1}^n P(y_i|\mathbf{x}_i,\theta)$$&lt;p>为便于计算，通常取对数转换乘积为求和:&lt;/p>
$$\log P(D|\theta) = \sum_{i=1}^n \log P(y_i|\mathbf{x}_i,\theta)$$&lt;p>这正是我们熟悉的交叉熵损失函数的负值。&lt;/p>
&lt;h3 id="最大后验估计-map">最大后验估计 (MAP)
&lt;/h3>&lt;p>贝叶斯学派则考虑参数 $\theta$ 的先验分布，采用最大后验估计:&lt;/p>
$$\theta_{\text{MAP}} = \arg\max_\theta P(\theta|D) = \arg\max_\theta \frac{P(D|\theta)P(\theta)}{P(D)}$$&lt;p>由于 $P(D)$ 对参数优化而言是常数，简化为:&lt;/p>
$$\theta_{\text{MAP}} = \arg\max_\theta P(D|\theta)P(\theta)$$&lt;p>取对数后:&lt;/p>
$$\theta_{\text{MAP}} = \arg\max_\theta \left[ \log P(D|\theta) + \log P(\theta) \right]$$&lt;p>可以看出，MAP 比 MLE 多了先验项 $\log P(\theta)$，这实际上起到了正则化作用。例如，当先验为高斯分布 $P(\theta) \sim \mathcal{N}(0, \sigma^2)$ 时，$\log P(\theta)$ 对应于 $L_2$ 正则化；当先验为拉普拉斯分布时，对应于 $L_1$ 正则化。&lt;/p>
&lt;h3 id="完全贝叶斯推断">完全贝叶斯推断
&lt;/h3>&lt;p>更进一步，完全贝叶斯推断不仅寻找单一最优参数点，而是&lt;strong>考虑所有可能参数及其概率分布&lt;/strong>:&lt;/p>
$$P(y|\mathbf{x}, D) = \int P(y|\mathbf{x}, \theta) P(\theta|D) d\theta$$&lt;p>这个积分通常难以解析计算，需要借助变分推断 (Variational Inference) 或马尔可夫链蒙特卡洛 (MCMC) 等近似方法。完全贝叶斯推断的优势在于能够量化预测的不确定性，而不仅仅给出点估计。&lt;/p>
&lt;h2 id="生成式模型">生成式模型
&lt;/h2>&lt;p>我们使用的大模型和上文的判别式模型有一个本质区别就是: &lt;strong>判别式模型给定输入 x 的情况下, 输出的 lable 是确定的&lt;/strong>, 但是大模型每次生成文本都有不同之处, 这是因为实际上每次大模型生成的是下一个词语的概率分布, 然后按照某种方式在其中采样, 我们把这种模型叫做&lt;strong>生成式模型&lt;/strong>&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-1.png"
width="1388"
height="733"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-1_hu_cc14b7d08b7e7759.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-1_hu_8b236c2fb61ef3dd.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-1"
class="gallery-image"
data-flex-grow="189"
data-flex-basis="454px"
>&lt;/p>
&lt;p>生成式模型特点&lt;/p>
&lt;ul>
&lt;li>一个输入多个输出：同一个输入可能对应多种合理的输出结果&lt;/li>
&lt;li>训练数据中没有确切的解：不像分类任务有唯一正确答案&lt;/li>
&lt;li>难以预测：输出空间通常非常大且复杂&lt;/li>
&lt;/ul>
&lt;p>生成式模型的核心目标是学习数据的分布，而不仅仅是将输入映射到特定输出。这使得它们能够生成新的、多样化的、符合真实数据分布的样本。&lt;/p>
&lt;h3 id="判别式-vs-生成式">判别式 vs 生成式
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-2.png"
width="1379"
height="789"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-2_hu_b0f975d65376da0f.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-2_hu_603ebab5390673c4.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-2"
class="gallery-image"
data-flex-grow="174"
data-flex-basis="419px"
>&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-3.png"
width="1360"
height="746"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-3_hu_9ddcec929cac0dfc.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-3_hu_59a37c7c120a6365.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-3"
class="gallery-image"
data-flex-grow="182"
data-flex-basis="437px"
>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>判别式模型: $P(y|x)$ - 给定输入 x，预测标签 y 的概率&lt;/p>
&lt;ul>
&lt;li>例如：分类器、回归模型&lt;/li>
&lt;li>关注决策边界，区分不同类别&lt;/li>
&lt;li>通常计算效率更高，需要的数据更少&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>生成式模型: $P(x|y)$ 或 $P(x,y)$ 或 $P(x)$, 给定输入标签 y，生成一个最可能符合现实中数据分布的 x&lt;/p>
&lt;ul>
&lt;li>学习数据本身的联合分布&lt;/li>
&lt;li>能够生成新样本&lt;/li>
&lt;li>通常需要更多参数和训练数据&lt;/li>
&lt;li>提供更丰富的信息（联合概率分布）&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h3 id="概率建模">概率建模
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-4.png"
width="1403"
height="745"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-4_hu_b844b71235ccf4e5.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-4_hu_2fc144657aa7cef5.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-4"
class="gallery-image"
data-flex-grow="188"
data-flex-basis="451px"
>&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>潜在因子（Latent Factors）&lt;/strong>：用 z 表示隐藏变量，如物体的姿势（pose）、光照（lighting）、尺度（scale）等。这些因子本身服从简单分布。&lt;/p>
&lt;ul>
&lt;li>潜在空间通常是低维的，具有良好的结构&lt;/li>
&lt;li>潜在变量捕获了数据生成过程中的关键因素&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>观察值生成&lt;/strong>：观察数据 x（如图像）由 &amp;ldquo;世界模型&amp;rdquo; 渲染生成，而世界模型是关于 z 的函数。最终，观察值 x 会呈现复杂分布。&lt;/p>
&lt;ul>
&lt;li>世界模型可以看作是从简单分布到复杂分布的变换&lt;/li>
&lt;li>深度神经网络可以作为这种变换的强大近似器&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>我们通过深度学习对整个数据的分布进行建模, 使得可以采样出符合真实世界的数据 z, 而这个过程的关键就是概率&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>数学表示&lt;/strong>：&lt;/p>
&lt;p>&lt;strong>生成模型的核心思想是通过联合分布 P (x, z) 来求解数据的边缘分布 P (x)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>联合分布：$P(x,z) = P(x|z)P(z)$&lt;/li>
&lt;li>边缘分布：$P(x) = \int P(x|z)P(z)dz$&lt;/li>
&lt;li>后验分布：$P(z|x) = \frac{P(x|z)P(z)}{P(x)}$&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-5.png"
width="1361"
height="742"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-5_hu_8ba72b7b61144ef8.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-5_hu_4f821e14ea1a60d3.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-5"
class="gallery-image"
data-flex-grow="183"
data-flex-basis="440px"
>&lt;/p>
&lt;p>对估计的概率分布和数据的分布进行对比, 作为损失函数。常用的度量方式包括：&lt;/p>
&lt;ul>
&lt;li>KL 散度：$D_{KL}(P_{data}||P_{model})$&lt;/li>
&lt;li>最大似然估计：最大化 $\log P_{model}(x)$&lt;/li>
&lt;li>Wasserstein 距离：用于 WGAN 等模型&lt;/li>
&lt;/ul>
&lt;h2 id="deep-generative-models">Deep Generative Models
&lt;/h2>&lt;h3 id="表征学习">表征学习
&lt;/h3>&lt;p>深度学习的核心任务之一是表征学习, 即在原始数据中自动提取对任务有用的特征（即“表示”），而无需人工设计特征。&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-6.png"
width="1395"
height="779"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-6_hu_8ccbc38657ffc11c.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-6_hu_a5e2fd975a68e7af.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-6"
class="gallery-image"
data-flex-grow="179"
data-flex-basis="429px"
>&lt;/p>
&lt;p>为什么深度学习有用? 因为它的&lt;strong>分层结构&lt;/strong>可以自动学习数据的&lt;strong>层次化表示&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>低层特征：边缘、纹理、简单形状&lt;/li>
&lt;li>中层特征：部件、组合结构&lt;/li>
&lt;li>高层特征：语义概念、抽象表示&lt;/li>
&lt;/ul>
&lt;p>这种表示学习能力使深度学习模型能够捕获数据中的复杂模式。&lt;/p>
&lt;h3 id="建模概率分布">建模概率分布
&lt;/h3>&lt;p>既然深度学习可以通过表征学习去学习数据的信息, 那么他当然也可以学习数据的分布, 一种直观的的方式是, 我们通过模型的学习把简单分布建模为复杂分布&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-7.png"
width="1391"
height="715"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-7_hu_6413a9af038906de.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-7_hu_519c2e167c600b5c.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-7"
class="gallery-image"
data-flex-grow="194"
data-flex-basis="466px"
>&lt;/p>
&lt;ul>
&lt;li>使用神经网络参数化概率分布&lt;/li>
&lt;li>将简单的先验分布（如高斯分布）转换为复杂的数据分布&lt;/li>
&lt;li>学习数据的隐含结构和生成过程&lt;/li>
&lt;/ul>
&lt;p>POV:&lt;/p>
&lt;ul>
&lt;li>生成模型会将一些深度神经网络作为构建模块。&lt;/li>
&lt;li>就像深度神经网络会将某些 &amp;ldquo;层&amp;rdquo; 作为构建模块一样。&lt;/li>
&lt;li>生成模型是更高层级的抽象。&lt;/li>
&lt;/ul>
&lt;h3 id="使用方法">使用方法
&lt;/h3>&lt;p>生成式模型的核心是学习从条件信息 y 到目标数据 x 的映射过程：$P(x|y)$&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-8.png"
width="1340"
height="678"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-8_hu_91fcc7bd1df59c32.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-8_hu_b64014cac397bb3a.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-8"
class="gallery-image"
data-flex-grow="197"
data-flex-basis="474px"
>&lt;/p>
&lt;p>条件信息 y 在实际应用中可以是多种形式：&lt;/p>
&lt;ul>
&lt;li>&lt;strong>文本描述&lt;/strong>：如&amp;quot;一只橙色的猫坐在窗台上&amp;quot;（文生图）&lt;/li>
&lt;li>&lt;strong>类别标签&lt;/strong>：如数字&amp;quot;7&amp;quot;（条件图像生成）&lt;/li>
&lt;li>&lt;strong>属性向量&lt;/strong>：如年龄、性别、表情等（人脸生成）&lt;/li>
&lt;li>&lt;strong>部分数据&lt;/strong>：如图像的一部分（图像补全）&lt;/li>
&lt;li>&lt;strong>其他模态数据&lt;/strong>：如音频、视频片段（跨模态生成）&lt;/li>
&lt;/ul>
&lt;p>本质上，y 是对生成空间的约束，它提供了低熵、高抽象的信息，而模型则负责将这些约束转化为高熵、高细节的具体数据 x。这种从抽象到具体的映射过程，使生成式模型能够在保持一致性的同时产生多样化的输出。&lt;/p>
&lt;p>深度生成模型的主要应用场景：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>数据生成&lt;/strong>：创建新的、逼真的样本&lt;/li>
&lt;li>&lt;strong>数据增强&lt;/strong>：为监督学习任务生成额外训练数据&lt;/li>
&lt;li>&lt;strong>异常检测&lt;/strong>：识别不符合学习分布的样本&lt;/li>
&lt;li>&lt;strong>缺失数据填补&lt;/strong>：根据部分观察推断完整数据&lt;/li>
&lt;li>&lt;strong>压缩表示&lt;/strong>：学习数据的紧凑编码&lt;/li>
&lt;/ol>
&lt;h2 id="主要生成模型类型">主要生成模型类型
&lt;/h2>&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-9.png"
width="1250"
height="740"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-9_hu_67503cf93b69e0c0.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-9_hu_137b3a870ad7c88f.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-9"
class="gallery-image"
data-flex-grow="168"
data-flex-basis="405px"
>&lt;/p>
&lt;h3 id="自回归模型-autoregressive-models">自回归模型 (Autoregressive Models)
&lt;/h3>&lt;p>自回归模型将联合分布分解为条件概率的乘积：
$P(x) = \prod_{i=1}^{n} P(x_i|x_{&amp;lt;i})$&lt;/p>
&lt;p>特点：&lt;/p>
&lt;ul>
&lt;li>显式密度模型，可以直接计算似然&lt;/li>
&lt;li>生成过程是顺序的，每次生成一个元素&lt;/li>
&lt;li>代表模型：PixelRNN, PixelCNN, WaveNet, 语言模型&lt;/li>
&lt;li>最近特别强大的 GPT4o 就使用了自回归模型而非 diffusion model 来生成图像, 效果极其恐怖&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-10.jpg"
width="1024"
height="1536"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-10_hu_8f94989c5932e7f8.jpg 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-10_hu_d77d944712821c0.jpg 1024w"
loading="lazy"
alt="Bayesian-AI-image-10"
class="gallery-image"
data-flex-grow="66"
data-flex-basis="160px"
>&lt;/p>
&lt;h3 id="变分自编码器-vae">变分自编码器 (VAE)
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-11.png"
width="1558"
height="800"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-11_hu_18b6293ce7732e5.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-11_hu_1c08c2fb6df3acf1.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-11"
class="gallery-image"
data-flex-grow="194"
data-flex-basis="467px"
>&lt;/p>
&lt;p>VAE 通过变分推断学习潜在变量模型, 直接对一个图片建立一个概率分布：&lt;/p>
&lt;ul>
&lt;li>编码器网络：$q_\phi(z|x)$ 近似后验分布&lt;/li>
&lt;li>解码器网络：$p_\theta(x|z)$ 从潜在变量重建数据&lt;/li>
&lt;li>目标函数：ELBO (Evidence Lower BOund)
$$\mathcal{L}(\theta,\phi;x) = \mathbb{E}_{q_\phi(z|x)}[\log p_\theta(x|z)] - D_{KL}(q_\phi(z|x)||p(z))$$&lt;/li>
&lt;/ul>
&lt;p>特点：&lt;/p>
&lt;ul>
&lt;li>学习有意义的潜在空间&lt;/li>
&lt;li>生成质量通常不如 GAN&lt;/li>
&lt;li>训练稳定，避免模式崩溃&lt;/li>
&lt;li>后续成为了 stable diffusion 的一个模块&lt;/li>
&lt;/ul>
&lt;h3 id="生成对抗网络-gan">生成对抗网络 (GAN)
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-12.png"
width="1528"
height="882"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-12_hu_fb0e9359eb1c0194.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-12_hu_18a94bc5e547e577.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-12"
class="gallery-image"
data-flex-grow="173"
data-flex-basis="415px"
>&lt;/p>
&lt;p>GAN 通过博弈论框架学习生成模型：&lt;/p>
&lt;ul>
&lt;li>生成器 G：创建看起来真实的样本&lt;/li>
&lt;li>判别器 D：区分真实样本和生成样本&lt;/li>
&lt;li>目标函数：&lt;/li>
&lt;/ul>
$$\min_G \max_D V(D,G) = \mathbb{E}_{x\sim p_{data}}[\log D(x)] + \mathbb{E}_{z\sim p_z}[\log(1-D(G(z)))]$$&lt;p>特点：&lt;/p>
&lt;ul>
&lt;li>生成质量极高&lt;/li>
&lt;li>训练不稳定，容易模式崩溃&lt;/li>
&lt;li>难以评估模型质量&lt;/li>
&lt;li>变种众多：DCGAN, WGAN, StyleGAN 等&lt;/li>
&lt;/ul>
&lt;p>效果图:&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-13.png"
width="636"
height="638"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-13_hu_fc381913958a240c.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-13_hu_74e1b8b55d27c21d.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-13"
class="gallery-image"
data-flex-grow="99"
data-flex-basis="239px"
> image 20250330101500. Png&lt;/p>
&lt;h3 id="扩散模型-diffusion-models">扩散模型 (Diffusion Models)
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-14.png"
width="1567"
height="877"
srcset="https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-14_hu_b1ace60cfff951ec.png 480w, https://tok1024.com/p/bayesian-ai/images/Bayesian%20AI-image-14_hu_36e5758cc264f73f.png 1024w"
loading="lazy"
alt="Bayesian-AI-image-14"
class="gallery-image"
data-flex-grow="178"
data-flex-basis="428px"
>&lt;/p>
&lt;p>扩散模型通过逐步去噪学习生成过程：&lt;/p>
&lt;ul>
&lt;li>前向过程：逐步向数据添加噪声&lt;/li>
&lt;li>反向过程：学习去噪，恢复原始数据&lt;/li>
&lt;li>目标函数：预测添加的噪声&lt;/li>
&lt;/ul>
&lt;p>特点：&lt;/p>
&lt;ul>
&lt;li>生成质量超越 GAN&lt;/li>
&lt;li>训练稳定&lt;/li>
&lt;li>推理速度较慢&lt;/li>
&lt;li>代表模型：DDPM, DALL-E 2, Stable Diffusion&lt;/li>
&lt;/ul>
&lt;h2 id="总结">总结
&lt;/h2>&lt;p>本文通过贝叶斯视角揭示了现代AI的核心本质：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>贝叶斯思想的革命性影响&lt;/strong>：贝叶斯学派将概率从客观频率重新定义为不确定性的度量，这一转变是现代AI能够处理复杂、不确定世界的理论基础。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>判别式到生成式的范式转变&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>判别式模型（$P(y|x)$）只关注输入到输出的映射，本质上是确定性的&lt;/li>
&lt;li>生成式模型（$P(x,y)$或$P(x)$）学习数据本身的分布，能够产生多样化输出&lt;/li>
&lt;li>这一转变解释了为什么大模型能够对同一问题给出不同但合理的回答&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>潜变量与概率分布变换&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>引入潜变量$z$是现代生成模型的关键突破&lt;/li>
&lt;li>通过将复杂分布表示为简单分布的变换，AI获得了&amp;quot;创造性&amp;quot;&lt;/li>
&lt;li>深度神经网络作为这种变换的强大近似器，使复杂分布的建模成为可能&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>采样机制的重要性&lt;/strong>：&lt;/p>
&lt;ul>
&lt;li>大模型不是简单地记忆和重复，而是从学习到的概率分布中采样&lt;/li>
&lt;li>这种机制使AI能够生成新颖且多样的内容，而非固定输出&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ol>
&lt;p>这种基于概率的视角不仅统一了从VAE到GAN再到扩散模型和自回归模型的技术路线，也为我们理解AI的能力边界提供了理论框架。正是贝叶斯思想的引入，使AI从简单的模式匹配工具进化为能够处理不确定性、具有创造性的智能系统。&lt;/p>
&lt;p>回到开篇的问题：大模型写的作业不会雷同，正是它们不是确定性地输出&amp;quot;最优解&amp;quot;，而是从学习到的概率分布中进行采样，这种基于概率的生成机制是现代AI最本质的特征。&lt;/p></description></item><item><title>卷积</title><link>https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/</link><pubDate>Fri, 21 Mar 2025 23:25:38 +0800</pubDate><guid>https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/</guid><description>&lt;h2 id="概念">概念
&lt;/h2>&lt;p>首先, 什么是卷积?&lt;/p>
&lt;p>卷积是一种特殊的积分变换，它通过以下步骤将两个函数 f 和 g 组合：&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-1.png"
width="2393"
height="1169"
srcset="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-1_hu_7a66c9aea83fc3da.png 480w, https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-1_hu_f71053dc48cf9670.png 1024w"
loading="lazy"
alt="卷积-image-1"
class="gallery-image"
data-flex-grow="204"
data-flex-basis="491px"
>&lt;/p>
&lt;ol>
&lt;li>将 g 函数翻折（$g (τ)→g (-τ)$）&lt;/li>
&lt;li>对于输出的每个时间点 t，将翻折后的 g 函数&lt;strong>平移&lt;/strong>到 t 位置（g (-τ)→g (t-τ)）&lt;/li>
&lt;li>计算 $f (τ)$ 与平移后的 $g (t-τ)$ 的乘积&lt;/li>
&lt;li>对所有τ积分/求和，得到输出点 t 的值&lt;/li>
&lt;/ol>
&lt;p>数学表达为：&lt;/p>
&lt;ul>
&lt;li>连续情况：$(f * g)(t)$ = $∫f (τ) g (t-τ) dτ$&lt;/li>
&lt;li>离散情况：$(f * g)[n] = ∑f[k]g[n-k]$&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-2.png"
width="2507"
height="891"
srcset="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-2_hu_37973905ef3732b9.png 480w, https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-2_hu_755391ee44ea6f02.png 1024w"
loading="lazy"
alt="卷积-image-2"
class="gallery-image"
data-flex-grow="281"
data-flex-basis="675px"
>&lt;/p>
&lt;h2 id="分析">分析
&lt;/h2>&lt;h3 id="积分角度">积分角度
&lt;/h3>&lt;p>为什么要进行这些操作? 让我们以用两个函数的叠加为背景来分析每个操作的合理性&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-3.png"
width="2438"
height="1312"
srcset="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-3_hu_2d84c226477b4b39.png 480w, https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-3_hu_34632cf0e722794e.png 1024w"
loading="lazy"
alt="卷积-image-3"
class="gallery-image"
data-flex-grow="185"
data-flex-basis="445px"
>&lt;/p>
&lt;p>首先需要确定的是: &lt;strong>卷积中的自变量是 t, 而非 x&lt;/strong>, 观察公式&lt;/p>
$$y(t) = \int_{-\infty}^{\infty} f(\tau)g(t-\tau)d\tau$$&lt;ul>
&lt;li>T 是我们关心的当前时刻&lt;/li>
&lt;li>τ 是过去的某个时刻&lt;/li>
&lt;li>T-τ 表示&amp;quot;从过去时刻τ到当前时刻 t 的时间差&lt;/li>
&lt;/ul>
&lt;p>也就是说我们实际上做的积分是: &lt;strong>过去的每一个时间 τ 的响应叠加起来, 会对当前时间 t 有多大的影响&lt;/strong>, 这样看下来是不是简单多了呢?&lt;/p>
&lt;h3 id="翻折平移">翻折平移
&lt;/h3>&lt;p>让我们继续换一个视角, 从函数翻折和平移的角度来思考&lt;/p>
&lt;p>$g(t-\tau)$ 这个公式从几何角度如何理解? 先把函数&lt;strong>向左(时间提前)&lt;/strong> 平移 t 个单位, 然后再左右翻折&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-5.png"
width="2546"
height="1429"
srcset="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-5_hu_57e730930021a8b5.png 480w, https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-5_hu_6e865e843201280e.png 1024w"
loading="lazy"
alt="卷积-image-5"
class="gallery-image"
data-flex-grow="178"
data-flex-basis="427px"
>&lt;/p>
&lt;p>让我们继续换一个视角，从函数翻折和平移的角度来思考卷积的几何意义。&lt;/p>
&lt;p>$g(t-\tau)$ 这个表达式从几何角度如何理解？实际上是先将函数 $g(\tau)$ &lt;strong>左右翻折&lt;/strong>得到 $g(-\tau)$，然后再&lt;strong>向右(时间延迟)&lt;/strong> 平移 $t$ 个单位得到 $g(t-\tau)$。&lt;/p>
&lt;p>&lt;img src="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-5.png"
width="2546"
height="1429"
srcset="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-5_hu_57e730930021a8b5.png 480w, https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-5_hu_6e865e843201280e.png 1024w"
loading="lazy"
alt="卷积-image-5"
class="gallery-image"
data-flex-grow="178"
data-flex-basis="427px"
>&lt;/p>
&lt;p>这个过程可以这样直观理解：想象 $g(x)$ 是一个&lt;strong>滑动的窗口或模板&lt;/strong>，我们将这个窗口沿着 $f(x)$ 移动。对于每一个时间点 $t$，我们：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>将窗口 $g$ 翻转&lt;/p>
&lt;/li>
&lt;li>
&lt;p>将翻转后的窗口中心放在 $t$ 处&lt;/p>
&lt;/li>
&lt;li>
&lt;p>计算窗口与 $f(x)$ 的重叠部分（即它们的乘积）&lt;/p>
&lt;/li>
&lt;li>
&lt;p>对所有重叠部分求积分，得到卷积结果在 $t$ 处的值&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>为什么需要翻折？这是因为时间的对应关系。如果直接将 $f$ 和 $g$ 相乘而不翻折 $g$，时间对应关系会出现错误：$f$ 中较早的时间点（左侧）会与 $g$ 中较早的部分（也是左侧）对应。&lt;/p>
&lt;p>但在物理系统中，较早发生的输入应该与系统较晚的响应对应——因为输入信号需要时间才能通过系统产生响应。翻折操作正是为了建立这种正确的时间对应关系：$f$ 中较早的输入（左侧）会与翻折后的 $g$ 中较晚的响应（右侧）对应。&lt;/p>
&lt;p>这样，卷积操作 $g(t-\tau)$ 精确地捕捉了系统响应的时间演化特性，体现了&amp;quot;过去的输入如何影响当前的输出&amp;quot;这一物理本质。&lt;/p>
&lt;h3 id="齐次性">齐次性
&lt;/h3>&lt;p>卷积操作中的一个重要特性可以通过变量替换关系 $\tau + (t - \tau) = t$ 来理解。这个看似简单的等式实际上揭示了卷积的本质：&lt;/p>
&lt;p>在卷积积分 $y(t) = \int_{-\infty}^{\infty} f(\tau)g(t-\tau)d\tau$ 中：&lt;/p>
&lt;ul>
&lt;li>
&lt;p>$\tau$ 表示输入信号发生的时刻&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$t-\tau$ 表示从输入到当前时刻的时间差&lt;/p>
&lt;/li>
&lt;li>
&lt;p>$t$ 是我们关心的当前时刻&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>这个齐次关系 $\tau + (t - \tau) = t$ 体现了因果性和时间不变性：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>时间守恒&lt;/strong>：输入时刻加上延迟时间等于输出时刻，这是物理系统中时间流逝的自然表达&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>坐标变换不变性&lt;/strong>：无论我们如何选择时间原点，卷积操作的结果都保持不变，这反映了物理规律的普适性&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>线性时不变系统的本质&lt;/strong>：系统对输入的响应只取决于输入与当前时刻的时间差，而不依赖于绝对时间&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>从几何角度看，这种齐次性意味着卷积操作可以理解为一种&amp;quot;滑动加权平均&amp;quot;：随着时间点 $t$ 的变化，我们沿着时间轴滑动窗口 $g(t-\tau)$，但窗口的形状（即系统的响应特性）保持不变。&lt;/p>
&lt;p>这种齐次性也解释了为什么卷积在频域中对应简单的乘法：时域中的这种&amp;quot;滑动不变性&amp;quot;在频域中表现为各频率分量的独立处理，每个频率分量只受到一个固定因子的调制。&lt;/p>
&lt;h2 id="卷积有什么用">卷积有什么用
&lt;/h2>&lt;p>卷积作为一种基本的数学运算，在多个领域都有广泛应用。以下是几个重要的应用场景：&lt;/p>
&lt;h3 id="多项式乘法">多项式乘法
&lt;/h3>$$A(x) = a_0 + a_1x + a_2x^2 + ... + a_nx^n$$$$B(x) = b_0 + b_1x + b_2x^2 + ... + b_mx^m$$$$c_k = \sum_{i=0}^{k} a_i b_{k-i}$$&lt;p>这正是离散卷积的形式。例如，$(1+2x+x^2)(3+x)$ 的展开可以通过计算序列 $[1,2,1]$ 和 $[3,1]$ 的卷积得到 $[3,7,5,1]$，对应多项式 $3+7x+5x^2+x^3$。&lt;/p>
&lt;p>事实上, 把 x 换成 10, 这就是我们计算的竖式乘法, 所以&lt;del>我们小学二年级就已经学过卷积了&lt;/del>&lt;/p>
&lt;h3 id="离散随机变量之和">离散随机变量之和
&lt;/h3>&lt;p>当两个独立随机变量相加时，其概率分布是原本两个分布的卷积。例如，掷两个骰子并求和：&lt;/p>
&lt;ul>
&lt;li>第一个骰子的概率分布：$P_X = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]$（对应1-6点）&lt;/li>
&lt;li>第二个骰子的概率分布：$P_Y = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]$&lt;/li>
&lt;li>两骰子和的概率分布：$P_{X+Y} = P_X * P_Y$&lt;/li>
&lt;/ul>
&lt;p>计算结果为：$[1/36, 2/36, 3/36, 4/36, 5/36, 6/36, 5/36, 4/36, 3/36, 2/36, 1/36]$，对应和为2-12的概率。&lt;/p>
&lt;h3 id="中心极限定理">中心极限定理
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-6.png"
width="1455"
height="694"
srcset="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-6_hu_af27d4acb440690.png 480w, https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-6_hu_1d20c7c47d3ea327.png 1024w"
loading="lazy"
alt="卷积-image-6"
class="gallery-image"
data-flex-grow="209"
data-flex-basis="503px"
>&lt;/p>
&lt;p>中心极限定理与卷积有深刻联系。当我们将多个独立同分布的随机变量相加时，根据卷积的性质，其分布会逐渐接近正态分布。&lt;/p>
&lt;p>从数学角度看，这是因为多次卷积操作会使分布变得越来越&amp;quot;光滑&amp;quot;。具体来说：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>单个随机变量&lt;/strong>：假设我们有一个均匀分布的随机变量X，其概率密度函数是一个矩形。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>两个随机变量之和&lt;/strong>：X₁+X₂的分布是两个均匀分布的卷积，结果是一个三角形分布（也称为辛普森分布）。这已经比原始的矩形分布更加&amp;quot;圆滑&amp;quot;。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>三个随机变量之和&lt;/strong>：X₁+X₂+X₃的分布是三个均匀分布的卷积，或者说是均匀分布与三角形分布的卷积，结果是一个抛物线形状的分布，更接近钟形。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>更多随机变量之和&lt;/strong>：随着我们继续增加随机变量，每次卷积操作都会使分布变得更加平滑和对称，锐角被磨平，分布的中心部分变得更加突出。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h4 id="数学公式描述">数学公式描述
&lt;/h4>&lt;p>假设我们有n个独立同分布的随机变量 $X_1, X_2, &amp;hellip;, X_n$，每个变量的期望为μ，方差为σ²。定义它们的和为：&lt;/p>
$$S_n = X_1 + X_2 + ... + X_n$$&lt;p>根据概率论，$S_n$的期望和方差为：&lt;/p>
$$E[S_n] = n\mu$$$$Var[S_n] = n\sigma^2$$&lt;p>中心极限定理告诉我们，当n足够大时，$S_n$的标准化形式：&lt;/p>
$$Z_n = \frac{S_n - n\mu}{\sigma\sqrt{n}}$$&lt;p>的分布会收敛到标准正态分布N(0,1)：&lt;/p>
$$Z_n \xrightarrow{d} N(0,1)$$&lt;p>也就是说，当n足够大时，$S_n$近似服从正态分布：&lt;/p>
$$S_n \approx N(n\mu, n\sigma^2)$$&lt;h4 id="为什么会收敛到正态分布">为什么会收敛到正态分布？
&lt;/h4>&lt;p>从傅里叶变换的角度，这一现象可以通过特征函数来解释。随机变量X的特征函数定义为：&lt;/p>
$$\phi_X(t) = E[e^{itX}]$$&lt;p>对于独立随机变量的和，其特征函数是各个变量特征函数的乘积：&lt;/p>
$$\phi_{S_n}(t) = [\phi_X(t)]^n$$&lt;p>当n很大时，可以通过泰勒展开证明：&lt;/p>
$$\phi_{S_n}\left(\frac{t}{\sigma\sqrt{n}}\right) \approx e^{-\frac{t^2}{2}}$$&lt;p>而$e^{-\frac{t^2}{2}}$正是标准正态分布的特征函数。&lt;/p>
&lt;p>从卷积角度看，这相当于说：重复卷积同一个分布n次，结果会趋向于正态分布的形状。这是因为卷积在频域对应乘法，多次卷积会强化中频成分，抑制高频成分，使得分布变得光滑且集中。&lt;/p>
&lt;h3 id="卷积神经网络cnn">卷积神经网络(CNN)
&lt;/h3>&lt;p>&lt;img src="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-7.png"
width="1411"
height="979"
srcset="https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-7_hu_e6268e5608abe9fc.png 480w, https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/images/%E5%8D%B7%E7%A7%AF-image-7_hu_b73bdb1c55134633.png 1024w"
loading="lazy"
alt="卷积-image-7"
class="gallery-image"
data-flex-grow="144"
data-flex-basis="345px"
>&lt;/p>
&lt;p>在深度学习中，卷积神经网络利用卷积操作处理图像、语音等数据。CNN中的卷积层执行的操作是：&lt;/p>
$$O[i,j] = \sum_{m}\sum_{n} I[i+m, j+n] \cdot K[m,n]$$&lt;p>其中：&lt;/p>
&lt;ul>
&lt;li>$I$ 是输入（如图像）&lt;/li>
&lt;li>$K$ 是卷积核（可学习的权重矩阵）&lt;/li>
&lt;li>$O$ 是输出特征图&lt;/li>
&lt;/ul>
&lt;p>卷积操作使CNN具有以下特性：&lt;/p>
&lt;ol>
&lt;li>&lt;strong>局部感受&lt;/strong>：每个神经元只关注输入的一小部分&lt;/li>
&lt;li>&lt;strong>参数共享&lt;/strong>：同一个卷积核在整个输入上滑动，大大减少参数数量&lt;/li>
&lt;li>&lt;strong>平移不变性&lt;/strong>：无论特征在输入中的位置如何，都能被相同的卷积核检测到&lt;/li>
&lt;/ol>
&lt;p>这些特性使CNN在图像识别、物体检测等任务中表现出色，成为计算机视觉领域的基础模型。&lt;/p>
&lt;p>虽然话是这么说, 但我感觉卷积和CNN的关系就和老婆饼跟老婆的关系差不多&amp;hellip;&lt;/p>
&lt;h2 id="卷积的深层洞见">卷积的深层洞见
&lt;/h2>&lt;p>除了前文讨论的内容，卷积还有一些更深层次的洞见值得探索：&lt;/p>
&lt;h3 id="对偶性与不确定性">对偶性与不确定性
&lt;/h3>&lt;p>卷积与傅里叶变换之间存在着深刻的对偶关系，这种关系揭示了信号在时域和频域的基本约束：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>时域展宽，频域压缩&lt;/strong>：当我们对信号进行卷积（如用高斯函数平滑）时，时域上的信号变得更宽，而其频谱则变得更窄。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>不确定性原理&lt;/strong>：这种对偶性直接导致了信号处理中的不确定性原理——信号不可能同时在时域和频域上无限集中，这与量子力学中的海森堡不确定性原理有着相似的数学形式。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="卷积定理的普适性">卷积定理的普适性
&lt;/h3>&lt;p>卷积定理（时域卷积等价于频域乘积）不仅适用于傅里叶变换，还适用于许多其他变换：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>拉普拉斯变换&lt;/strong>：时域卷积对应s域乘积&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>Z变换&lt;/strong>：序列卷积对应z域乘积&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>小波变换&lt;/strong>：在某些条件下也满足类似性质&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>这种普适性表明卷积作为一种操作，在数学上具有深刻的内在结构。&lt;/p>
&lt;h3 id="群论视角">群论视角
&lt;/h3>&lt;p>从抽象代数角度看，卷积可以被理解为群上的一种自然运算：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>群卷积&lt;/strong>：在群G上定义的函数f和g的卷积为：$(f * g)(x) = \int_G f(y)g(y^{-1}x)dy$&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>不变性&lt;/strong>：卷积天然保持群的作用不变性，这解释了为什么卷积在处理具有平移、旋转等对称性的数据时如此有效。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>群等变卷积网络&lt;/strong>：这一洞见已经推动了深度学习中群等变卷积网络的发展，使网络能够处理具有各种对称性的数据。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="信息论解释">信息论解释
&lt;/h3>&lt;p>从信息论角度，卷积可以被理解为：&lt;/p>
&lt;ol>
&lt;li>
&lt;p>&lt;strong>信息融合&lt;/strong>：卷积是一种最优的信息融合方式，在高斯噪声假设下，它等价于贝叶斯推断。&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>最大熵原理&lt;/strong>：在某些约束条件下，卷积产生的分布具有最大熵，这解释了为什么多次卷积会趋向正态分布。&lt;/p>
&lt;/li>
&lt;/ol>
&lt;h3 id="卷积定理时域卷积与频域乘积的对应关系">卷积定理：时域卷积与频域乘积的对应关系
&lt;/h3>&lt;p>让我们来探讨一个卷积的核心性质——卷积定理。这个定理告诉我们：&lt;strong>时域中的卷积等价于频域中的乘积&lt;/strong>。这听起来很神奇，但背后有着深刻的数学原理。&lt;/p>
&lt;p>从数学表达上看，卷积定理可以表示为：&lt;/p>
$$\mathcal{F}\{f * g\} = \mathcal{F}\{f\} \cdot \mathcal{F}\{g\}$$&lt;p>其中 $\mathcal{F}$ 表示傅里叶变换，$f * g$ 表示 $f$ 和 $g$ 的卷积。&lt;/p>
&lt;h4 id="为什么会这样">为什么会这样？
&lt;/h4>&lt;p>这种对应关系的本质可以从卷积的定义出发来理解。回顾卷积的定义：&lt;/p>
$$y(t) = \int_{-\infty}^{\infty} f(\tau)g(t-\tau)d\tau$$&lt;p>当我们对这个式子进行傅里叶变换时，会发生什么？&lt;/p>
&lt;p>傅里叶变换将时域信号分解为不同频率的正弦波的叠加。对于每个频率分量 $\omega$，卷积的傅里叶变换可以写为：&lt;/p>
$$\mathcal{F}\{f * g\}(\omega) = \int_{-\infty}^{\infty} \left( \int_{-\infty}^{\infty} f(\tau)g(t-\tau)d\tau \right) e^{-j\omega t} dt$$&lt;p>通过变换顺序和变量替换，可以证明这等价于 $F(\omega) \cdot G(\omega)$，其中 $F(\omega)$ 和 $G(\omega)$ 分别是 $f(t)$ 和 $g(t)$ 的傅里叶变换。&lt;/p>
&lt;h4 id="从滑动加权平均的角度理解">从滑动加权平均的角度理解
&lt;/h4>&lt;p>还记得我们之前讨论的&amp;quot;滑动加权平均&amp;quot;视角吗？卷积本质上是一种滑动窗口操作，窗口形状保持不变，只是位置在变化。&lt;/p>
&lt;p>在频域中，这种&amp;quot;滑动不变性&amp;quot;表现为什么呢？答案是：&lt;strong>频率分量的独立调制&lt;/strong>。&lt;/p>
&lt;p>每个频率分量都被独立地调整幅度和相位，而不会与其他频率产生&amp;quot;混淆&amp;quot;。这正是乘法的特性！对于频谱 $F(\omega)$ 中的每个分量，我们只需将其乘以 $G(\omega)$ 中对应频率的值即可。&lt;/p>
&lt;h4 id="齐次性的体现">齐次性的体现
&lt;/h4>&lt;p>这种对应关系也是卷积齐次性的一种体现。我们之前讨论过，卷积中的齐次关系 $\tau + (t - \tau) = t$ 体现了时间不变性。在频域中，这种不变性转化为频率分量的独立处理。&lt;/p>
&lt;p>时域中的&amp;quot;滑动不变性&amp;quot;意味着系统对输入的响应只取决于输入与当前时刻的时间差，而不依赖于绝对时间。这种性质在频域中自然对应为各频率分量的独立调制。&lt;/p>
&lt;h4 id="计算效率的提升">计算效率的提升
&lt;/h4>&lt;p>这种对偶性不仅具有理论意义，还带来了实际的计算优势。对于长序列的卷积，直接计算的复杂度是 $O(n^2)$，而利用快速傅里叶变换 (FFT)，我们可以将复杂度降低到 $O(n\log n)$：&lt;/p>
&lt;ol>
&lt;li>对输入信号进行 FFT&lt;/li>
&lt;li>在频域中相乘&lt;/li>
&lt;li>进行逆 FFT 得到结果&lt;/li>
&lt;/ol>
&lt;p>这种&amp;quot;变换-乘积-逆变换&amp;quot;的策略大大提高了卷积的计算效率，在信号处理、图像处理等领域有着广泛应用。&lt;/p>
&lt;p>总的来说，时域卷积等于频域乘积这一性质，揭示了卷积作为一种数学操作的内在优雅性，它将时域中复杂的积分操作转化为频域中简单的乘法，体现了数学中常见的&amp;quot;复杂问题简单化&amp;quot;的美妙转换。&lt;/p>
&lt;h2 id="总结">总结
&lt;/h2>&lt;p>卷积本质上是一种时空融合的数学语言，通过&amp;quot;滑动加权平均&amp;quot;将两个函数的全局信息深度融合：在时域体现为系统对&lt;strong>历史输入的累积响应&lt;/strong>，在频域展现为&lt;strong>频率分量的优雅调制&lt;/strong>，在概率论中呈现为&lt;strong>随机涨落的光滑收敛&lt;/strong>，在深度学习中则演化为&lt;strong>特征提取的通用范式&lt;/strong>。这种独特的组合方式——&lt;strong>以函数为权重、以积分为纽带、以对称为灵魂&lt;/strong>——使其成为贯通信号处理、概率论、物理学和人工智能的基础性思维工具，既刻画着自然界的因果律动，也驱动着现代科技的智能演进。(本段总结由 Deepseek 生成)&lt;/p></description></item></channel></rss>