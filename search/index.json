[{"content":"从贝叶斯视角理解 AI 引言: 大模型写的作业会雷同吗? 你有没有想过这样一个问题: 当你和室友一起把某门水课的大作业题目复制到大模型中时, 他会不会每次都产生完全一模一样的答案 ? 如果不会, 为什么呢?\n答案当然是不会。下面我将从概率论和贝叶斯视角出发，逐步解释为什么大模型不会生成完全相同的答案，以及为什么这种随机性实际上是一种优势。\n条件概率与贝叶斯思想 贝叶斯公式 首先让我们回顾概率论中的贝叶斯公式:\n$$P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}$$这个公式看似简单，却蕴含了贝叶斯学派的整个世界观。它不仅是一个计算条件概率的工具，更是一种思考和推理的方式。\n先验与后验 贝叶斯学派认为: 概率不是客观频率，而是对不确定性的量化表达；学习是一个不断用新证据更新信念的过程。简言之，世界上没有绝对确定的事物，我们通过持续观察获取新证据，不断更新对世界的认知。\n从贝叶斯公式的角度理解，若将 $A$ 视为假设，$B$ 视为观测到的证据，则:\n$P(A)$ 是先验概率 (Prior)：在观测到证据 $B$ 之前，我们对假设 $A$ 成立概率的初始估计 $P(A|B)$ 是后验概率 (Posterior)：在观测到证据 $B$ 之后，我们对假设 $A$ 成立概率的更新估计 $P(B|A)$ 是似然 (Likelihood)：假设 $A$ 成立的条件下，观测到证据 $B$ 的概率 $P(B)$ 是边缘概率 (Marginal Probability)：观测到证据 $B$ 的总体概率 用更直观的表达式:\n$$P(\\text{假设}|\\text{证据}) = \\frac{P(\\text{证据}|\\text{假设}) \\times P(\\text{假设})}{P(\\text{证据})}$$即: 后验 = 似然 × 先验 ÷ 证据概率\n频率学派 vs. 贝叶斯学派 贝叶斯学派与频率学派在概率解释和统计推断方面存在根本差异:\n频率学派 贝叶斯学派 概率表示事件的长期频率 概率表示信念的主观程度 参数是固定但未知的常数 参数是具有概率分布的随机变量 依赖 $p$ 值和置信区间 使用后验分布和可信区间 不使用先验信息 明确纳入先验信息 主要方法：最大似然估计 (MLE) 主要方法：最大后验估计 (MAP)、完全后验分布 判别式模型 以图像分类为例，模型预测过程涉及三个主体：模型参数 $\\theta$、输入数据 $\\mathbf{x}$（及其真实标签 $y$）和预测结果 $\\hat{y}$。从概率角度看，预测过程就是在给定输入 $\\mathbf{x}$ 的条件下，找出使条件概率 $P(y|\\mathbf{x},\\theta)$ 最大的类别 $\\hat{y}$:\n$$\\hat{y} = \\arg\\max_y P(y|\\mathbf{x},\\theta)$$关键问题是：如何确定最优的模型参数 $\\theta$？这就涉及到不同的参数估计方法。\n最大似然估计 (MLE) 频率学派采用最大似然估计，寻找能最大化观测数据概率的参数 (换句话说: 最能解释数据集的参数):\n$$\\theta_{\\text{MLE}} = \\arg\\max_\\theta P(D|\\theta)$$其中 $D = {(\\mathbf{x}i, y_i)}{i=1}^n$ 是训练数据集。假设数据独立同分布 (i.i.d.)，似然函数可表示为:\n$$P(D|\\theta) = \\prod_{i=1}^n P(y_i|\\mathbf{x}_i,\\theta)$$为便于计算，通常取对数转换乘积为求和:\n$$\\log P(D|\\theta) = \\sum_{i=1}^n \\log P(y_i|\\mathbf{x}_i,\\theta)$$这正是我们熟悉的交叉熵损失函数的负值。\n最大后验估计 (MAP) 贝叶斯学派则考虑参数 $\\theta$ 的先验分布，采用最大后验估计:\n$$\\theta_{\\text{MAP}} = \\arg\\max_\\theta P(\\theta|D) = \\arg\\max_\\theta \\frac{P(D|\\theta)P(\\theta)}{P(D)}$$由于 $P(D)$ 对参数优化而言是常数，简化为:\n$$\\theta_{\\text{MAP}} = \\arg\\max_\\theta P(D|\\theta)P(\\theta)$$取对数后:\n$$\\theta_{\\text{MAP}} = \\arg\\max_\\theta \\left[ \\log P(D|\\theta) + \\log P(\\theta) \\right]$$可以看出，MAP 比 MLE 多了先验项 $\\log P(\\theta)$，这实际上起到了正则化作用。例如，当先验为高斯分布 $P(\\theta) \\sim \\mathcal{N}(0, \\sigma^2)$ 时，$\\log P(\\theta)$ 对应于 $L_2$ 正则化；当先验为拉普拉斯分布时，对应于 $L_1$ 正则化。\n完全贝叶斯推断 更进一步，完全贝叶斯推断不仅寻找单一最优参数点，而是考虑所有可能参数及其概率分布:\n$$P(y|\\mathbf{x}, D) = \\int P(y|\\mathbf{x}, \\theta) P(\\theta|D) d\\theta$$这个积分通常难以解析计算，需要借助变分推断 (Variational Inference) 或马尔可夫链蒙特卡洛 (MCMC) 等近似方法。完全贝叶斯推断的优势在于能够量化预测的不确定性，而不仅仅给出点估计。\n生成式模型 我们使用的大模型和上文的判别式模型有一个本质区别就是: 判别式模型给定输入 x 的情况下, 输出的 lable 是确定的, 但是大模型每次生成文本都有不同之处, 这是因为实际上每次大模型生成的是下一个词语的概率分布, 然后按照某种方式在其中采样, 我们把这种模型叫做生成式模型\n生成式模型特点\n一个输入多个输出：同一个输入可能对应多种合理的输出结果 训练数据中没有确切的解：不像分类任务有唯一正确答案 难以预测：输出空间通常非常大且复杂 生成式模型的核心目标是学习数据的分布，而不仅仅是将输入映射到特定输出。这使得它们能够生成新的、多样化的、符合真实数据分布的样本。\n判别式 vs 生成式 判别式模型: $P(y|x)$ - 给定输入 x，预测标签 y 的概率\n例如：分类器、回归模型 关注决策边界，区分不同类别 通常计算效率更高，需要的数据更少 生成式模型: $P(x|y)$ 或 $P(x,y)$ 或 $P(x)$, 给定输入标签 y，生成一个最可能符合现实中数据分布的 x\n学习数据本身的联合分布 能够生成新样本 通常需要更多参数和训练数据 提供更丰富的信息（联合概率分布） 概率建模 潜在因子（Latent Factors）：用 z 表示隐藏变量，如物体的姿势（pose）、光照（lighting）、尺度（scale）等。这些因子本身服从简单分布。\n潜在空间通常是低维的，具有良好的结构 潜在变量捕获了数据生成过程中的关键因素 观察值生成：观察数据 x（如图像）由 \u0026ldquo;世界模型\u0026rdquo; 渲染生成，而世界模型是关于 z 的函数。最终，观察值 x 会呈现复杂分布。\n世界模型可以看作是从简单分布到复杂分布的变换 深度神经网络可以作为这种变换的强大近似器 我们通过深度学习对整个数据的分布进行建模, 使得可以采样出符合真实世界的数据 z, 而这个过程的关键就是概率\n数学表示：\n生成模型的核心思想是通过联合分布 P (x, z) 来求解数据的边缘分布 P (x)\n联合分布：$P(x,z) = P(x|z)P(z)$ 边缘分布：$P(x) = \\int P(x|z)P(z)dz$ 后验分布：$P(z|x) = \\frac{P(x|z)P(z)}{P(x)}$ 对估计的概率分布和数据的分布进行对比, 作为损失函数。常用的度量方式包括：\nKL 散度：$D_{KL}(P_{data}||P_{model})$ 最大似然估计：最大化 $\\log P_{model}(x)$ Wasserstein 距离：用于 WGAN 等模型 Deep Generative Models 表征学习 深度学习的核心任务之一是表征学习, 即在原始数据中自动提取对任务有用的特征（即“表示”），而无需人工设计特征。\n为什么深度学习有用? 因为它的分层结构可以自动学习数据的层次化表示：\n低层特征：边缘、纹理、简单形状 中层特征：部件、组合结构 高层特征：语义概念、抽象表示 这种表示学习能力使深度学习模型能够捕获数据中的复杂模式。\n建模概率分布 既然深度学习可以通过表征学习去学习数据的信息, 那么他当然也可以学习数据的分布, 一种直观的的方式是, 我们通过模型的学习把简单分布建模为复杂分布\n使用神经网络参数化概率分布 将简单的先验分布（如高斯分布）转换为复杂的数据分布 学习数据的隐含结构和生成过程 POV:\n生成模型会将一些深度神经网络作为构建模块。 就像深度神经网络会将某些 \u0026ldquo;层\u0026rdquo; 作为构建模块一样。 生成模型是更高层级的抽象。 使用方法 生成式模型的核心是学习从条件信息 y 到目标数据 x 的映射过程：$P(x|y)$\n条件信息 y 在实际应用中可以是多种形式：\n文本描述：如\u0026quot;一只橙色的猫坐在窗台上\u0026quot;（文生图） 类别标签：如数字\u0026quot;7\u0026quot;（条件图像生成） 属性向量：如年龄、性别、表情等（人脸生成） 部分数据：如图像的一部分（图像补全） 其他模态数据：如音频、视频片段（跨模态生成） 本质上，y 是对生成空间的约束，它提供了低熵、高抽象的信息，而模型则负责将这些约束转化为高熵、高细节的具体数据 x。这种从抽象到具体的映射过程，使生成式模型能够在保持一致性的同时产生多样化的输出。\n深度生成模型的主要应用场景：\n数据生成：创建新的、逼真的样本 数据增强：为监督学习任务生成额外训练数据 异常检测：识别不符合学习分布的样本 缺失数据填补：根据部分观察推断完整数据 压缩表示：学习数据的紧凑编码 主要生成模型类型 自回归模型 (Autoregressive Models) 自回归模型将联合分布分解为条件概率的乘积： $P(x) = \\prod_{i=1}^{n} P(x_i|x_{\u0026lt;i})$\n特点：\n显式密度模型，可以直接计算似然 生成过程是顺序的，每次生成一个元素 代表模型：PixelRNN, PixelCNN, WaveNet, 语言模型 最近特别强大的 GPT4o 就使用了自回归模型而非 diffusion model 来生成图像, 效果极其恐怖 变分自编码器 (VAE) VAE 通过变分推断学习潜在变量模型, 直接对一个图片建立一个概率分布：\n编码器网络：$q_\\phi(z|x)$ 近似后验分布 解码器网络：$p_\\theta(x|z)$ 从潜在变量重建数据 目标函数：ELBO (Evidence Lower BOund) $$\\mathcal{L}(\\theta,\\phi;x) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x)||p(z))$$ 特点：\n学习有意义的潜在空间 生成质量通常不如 GAN 训练稳定，避免模式崩溃 后续成为了 stable diffusion 的一个模块 生成对抗网络 (GAN) GAN 通过博弈论框架学习生成模型：\n生成器 G：创建看起来真实的样本 判别器 D：区分真实样本和生成样本 目标函数： $$\\min_G \\max_D V(D,G) = \\mathbb{E}_{x\\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z\\sim p_z}[\\log(1-D(G(z)))]$$特点：\n生成质量极高 训练不稳定，容易模式崩溃 难以评估模型质量 变种众多：DCGAN, WGAN, StyleGAN 等 效果图:\nimage 20250330101500. Png\n扩散模型 (Diffusion Models) 扩散模型通过逐步去噪学习生成过程：\n前向过程：逐步向数据添加噪声 反向过程：学习去噪，恢复原始数据 目标函数：预测添加的噪声 特点：\n生成质量超越 GAN 训练稳定 推理速度较慢 代表模型：DDPM, DALL-E 2, Stable Diffusion 总结 本文通过贝叶斯视角揭示了现代AI的核心本质：\n贝叶斯思想的革命性影响：贝叶斯学派将概率从客观频率重新定义为不确定性的度量，这一转变是现代AI能够处理复杂、不确定世界的理论基础。\n判别式到生成式的范式转变：\n判别式模型（$P(y|x)$）只关注输入到输出的映射，本质上是确定性的 生成式模型（$P(x,y)$或$P(x)$）学习数据本身的分布，能够产生多样化输出 这一转变解释了为什么大模型能够对同一问题给出不同但合理的回答 潜变量与概率分布变换：\n引入潜变量$z$是现代生成模型的关键突破 通过将复杂分布表示为简单分布的变换，AI获得了\u0026quot;创造性\u0026quot; 深度神经网络作为这种变换的强大近似器，使复杂分布的建模成为可能 采样机制的重要性：\n大模型不是简单地记忆和重复，而是从学习到的概率分布中采样 这种机制使AI能够生成新颖且多样的内容，而非固定输出 这种基于概率的视角不仅统一了从VAE到GAN再到扩散模型和自回归模型的技术路线，也为我们理解AI的能力边界提供了理论框架。正是贝叶斯思想的引入，使AI从简单的模式匹配工具进化为能够处理不确定性、具有创造性的智能系统。\n回到开篇的问题：大模型写的作业不会雷同，正是它们不是确定性地输出\u0026quot;最优解\u0026quot;，而是从学习到的概率分布中进行采样，这种基于概率的生成机制是现代AI最本质的特征。\n","date":"2025-03-30T11:59:10+08:00","permalink":"https://tok1024.com/p/bayesian-ai/","title":"Bayesian Ai"},{"content":"概念 首先, 什么是卷积?\n卷积是一种特殊的积分变换，它通过以下步骤将两个函数 f 和 g 组合：\n将 g 函数翻折（$g (τ)→g (-τ)$） 对于输出的每个时间点 t，将翻折后的 g 函数平移到 t 位置（g (-τ)→g (t-τ)） 计算 $f (τ)$ 与平移后的 $g (t-τ)$ 的乘积 对所有τ积分/求和，得到输出点 t 的值 数学表达为：\n连续情况：$(f * g)(t)$ = $∫f (τ) g (t-τ) dτ$ 离散情况：$(f * g)[n] = ∑f[k]g[n-k]$ 分析 积分角度 为什么要进行这些操作? 让我们以用两个函数的叠加为背景来分析每个操作的合理性\n首先需要确定的是: 卷积中的自变量是 t, 而非 x, 观察公式\n$$y(t) = \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau)d\\tau$$ T 是我们关心的当前时刻 τ 是过去的某个时刻 T-τ 表示\u0026quot;从过去时刻τ到当前时刻 t 的时间差 也就是说我们实际上做的积分是: 过去的每一个时间 τ 的响应叠加起来, 会对当前时间 t 有多大的影响, 这样看下来是不是简单多了呢?\n翻折平移 让我们继续换一个视角, 从函数翻折和平移的角度来思考\n$g(t-\\tau)$ 这个公式从几何角度如何理解? 先把函数向左(时间提前) 平移 t 个单位, 然后再左右翻折\n让我们继续换一个视角，从函数翻折和平移的角度来思考卷积的几何意义。\n$g(t-\\tau)$ 这个表达式从几何角度如何理解？实际上是先将函数 $g(\\tau)$ 左右翻折得到 $g(-\\tau)$，然后再向右(时间延迟) 平移 $t$ 个单位得到 $g(t-\\tau)$。\n这个过程可以这样直观理解：想象 $g(x)$ 是一个滑动的窗口或模板，我们将这个窗口沿着 $f(x)$ 移动。对于每一个时间点 $t$，我们：\n将窗口 $g$ 翻转\n将翻转后的窗口中心放在 $t$ 处\n计算窗口与 $f(x)$ 的重叠部分（即它们的乘积）\n对所有重叠部分求积分，得到卷积结果在 $t$ 处的值\n为什么需要翻折？这是因为时间的对应关系。如果直接将 $f$ 和 $g$ 相乘而不翻折 $g$，时间对应关系会出现错误：$f$ 中较早的时间点（左侧）会与 $g$ 中较早的部分（也是左侧）对应。\n但在物理系统中，较早发生的输入应该与系统较晚的响应对应——因为输入信号需要时间才能通过系统产生响应。翻折操作正是为了建立这种正确的时间对应关系：$f$ 中较早的输入（左侧）会与翻折后的 $g$ 中较晚的响应（右侧）对应。\n这样，卷积操作 $g(t-\\tau)$ 精确地捕捉了系统响应的时间演化特性，体现了\u0026quot;过去的输入如何影响当前的输出\u0026quot;这一物理本质。\n齐次性 卷积操作中的一个重要特性可以通过变量替换关系 $\\tau + (t - \\tau) = t$ 来理解。这个看似简单的等式实际上揭示了卷积的本质：\n在卷积积分 $y(t) = \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau)d\\tau$ 中：\n$\\tau$ 表示输入信号发生的时刻\n$t-\\tau$ 表示从输入到当前时刻的时间差\n$t$ 是我们关心的当前时刻\n这个齐次关系 $\\tau + (t - \\tau) = t$ 体现了因果性和时间不变性：\n时间守恒：输入时刻加上延迟时间等于输出时刻，这是物理系统中时间流逝的自然表达\n坐标变换不变性：无论我们如何选择时间原点，卷积操作的结果都保持不变，这反映了物理规律的普适性\n线性时不变系统的本质：系统对输入的响应只取决于输入与当前时刻的时间差，而不依赖于绝对时间\n从几何角度看，这种齐次性意味着卷积操作可以理解为一种\u0026quot;滑动加权平均\u0026quot;：随着时间点 $t$ 的变化，我们沿着时间轴滑动窗口 $g(t-\\tau)$，但窗口的形状（即系统的响应特性）保持不变。\n这种齐次性也解释了为什么卷积在频域中对应简单的乘法：时域中的这种\u0026quot;滑动不变性\u0026quot;在频域中表现为各频率分量的独立处理，每个频率分量只受到一个固定因子的调制。\n卷积有什么用 卷积作为一种基本的数学运算，在多个领域都有广泛应用。以下是几个重要的应用场景：\n多项式乘法 $$A(x) = a_0 + a_1x + a_2x^2 + ... + a_nx^n$$$$B(x) = b_0 + b_1x + b_2x^2 + ... + b_mx^m$$$$c_k = \\sum_{i=0}^{k} a_i b_{k-i}$$这正是离散卷积的形式。例如，$(1+2x+x^2)(3+x)$ 的展开可以通过计算序列 $[1,2,1]$ 和 $[3,1]$ 的卷积得到 $[3,7,5,1]$，对应多项式 $3+7x+5x^2+x^3$。\n事实上, 把 x 换成 10, 这就是我们计算的竖式乘法, 所以我们小学二年级就已经学过卷积了\n离散随机变量之和 当两个独立随机变量相加时，其概率分布是原本两个分布的卷积。例如，掷两个骰子并求和：\n第一个骰子的概率分布：$P_X = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]$（对应1-6点） 第二个骰子的概率分布：$P_Y = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6]$ 两骰子和的概率分布：$P_{X+Y} = P_X * P_Y$ 计算结果为：$[1/36, 2/36, 3/36, 4/36, 5/36, 6/36, 5/36, 4/36, 3/36, 2/36, 1/36]$，对应和为2-12的概率。\n中心极限定理 中心极限定理与卷积有深刻联系。当我们将多个独立同分布的随机变量相加时，根据卷积的性质，其分布会逐渐接近正态分布。\n从数学角度看，这是因为多次卷积操作会使分布变得越来越\u0026quot;光滑\u0026quot;。具体来说：\n单个随机变量：假设我们有一个均匀分布的随机变量X，其概率密度函数是一个矩形。\n两个随机变量之和：X₁+X₂的分布是两个均匀分布的卷积，结果是一个三角形分布（也称为辛普森分布）。这已经比原始的矩形分布更加\u0026quot;圆滑\u0026quot;。\n三个随机变量之和：X₁+X₂+X₃的分布是三个均匀分布的卷积，或者说是均匀分布与三角形分布的卷积，结果是一个抛物线形状的分布，更接近钟形。\n更多随机变量之和：随着我们继续增加随机变量，每次卷积操作都会使分布变得更加平滑和对称，锐角被磨平，分布的中心部分变得更加突出。\n数学公式描述 假设我们有n个独立同分布的随机变量 $X_1, X_2, \u0026hellip;, X_n$，每个变量的期望为μ，方差为σ²。定义它们的和为：\n$$S_n = X_1 + X_2 + ... + X_n$$根据概率论，$S_n$的期望和方差为：\n$$E[S_n] = n\\mu$$$$Var[S_n] = n\\sigma^2$$中心极限定理告诉我们，当n足够大时，$S_n$的标准化形式：\n$$Z_n = \\frac{S_n - n\\mu}{\\sigma\\sqrt{n}}$$的分布会收敛到标准正态分布N(0,1)：\n$$Z_n \\xrightarrow{d} N(0,1)$$也就是说，当n足够大时，$S_n$近似服从正态分布：\n$$S_n \\approx N(n\\mu, n\\sigma^2)$$为什么会收敛到正态分布？ 从傅里叶变换的角度，这一现象可以通过特征函数来解释。随机变量X的特征函数定义为：\n$$\\phi_X(t) = E[e^{itX}]$$对于独立随机变量的和，其特征函数是各个变量特征函数的乘积：\n$$\\phi_{S_n}(t) = [\\phi_X(t)]^n$$当n很大时，可以通过泰勒展开证明：\n$$\\phi_{S_n}\\left(\\frac{t}{\\sigma\\sqrt{n}}\\right) \\approx e^{-\\frac{t^2}{2}}$$而$e^{-\\frac{t^2}{2}}$正是标准正态分布的特征函数。\n从卷积角度看，这相当于说：重复卷积同一个分布n次，结果会趋向于正态分布的形状。这是因为卷积在频域对应乘法，多次卷积会强化中频成分，抑制高频成分，使得分布变得光滑且集中。\n卷积神经网络(CNN) 在深度学习中，卷积神经网络利用卷积操作处理图像、语音等数据。CNN中的卷积层执行的操作是：\n$$O[i,j] = \\sum_{m}\\sum_{n} I[i+m, j+n] \\cdot K[m,n]$$其中：\n$I$ 是输入（如图像） $K$ 是卷积核（可学习的权重矩阵） $O$ 是输出特征图 卷积操作使CNN具有以下特性：\n局部感受：每个神经元只关注输入的一小部分 参数共享：同一个卷积核在整个输入上滑动，大大减少参数数量 平移不变性：无论特征在输入中的位置如何，都能被相同的卷积核检测到 这些特性使CNN在图像识别、物体检测等任务中表现出色，成为计算机视觉领域的基础模型。\n虽然话是这么说, 但我感觉卷积和CNN的关系就和老婆饼跟老婆的关系差不多\u0026hellip;\n卷积的深层洞见 除了前文讨论的内容，卷积还有一些更深层次的洞见值得探索：\n对偶性与不确定性 卷积与傅里叶变换之间存在着深刻的对偶关系，这种关系揭示了信号在时域和频域的基本约束：\n时域展宽，频域压缩：当我们对信号进行卷积（如用高斯函数平滑）时，时域上的信号变得更宽，而其频谱则变得更窄。\n不确定性原理：这种对偶性直接导致了信号处理中的不确定性原理——信号不可能同时在时域和频域上无限集中，这与量子力学中的海森堡不确定性原理有着相似的数学形式。\n卷积定理的普适性 卷积定理（时域卷积等价于频域乘积）不仅适用于傅里叶变换，还适用于许多其他变换：\n拉普拉斯变换：时域卷积对应s域乘积\nZ变换：序列卷积对应z域乘积\n小波变换：在某些条件下也满足类似性质\n这种普适性表明卷积作为一种操作，在数学上具有深刻的内在结构。\n群论视角 从抽象代数角度看，卷积可以被理解为群上的一种自然运算：\n群卷积：在群G上定义的函数f和g的卷积为：$(f * g)(x) = \\int_G f(y)g(y^{-1}x)dy$\n不变性：卷积天然保持群的作用不变性，这解释了为什么卷积在处理具有平移、旋转等对称性的数据时如此有效。\n群等变卷积网络：这一洞见已经推动了深度学习中群等变卷积网络的发展，使网络能够处理具有各种对称性的数据。\n信息论解释 从信息论角度，卷积可以被理解为：\n信息融合：卷积是一种最优的信息融合方式，在高斯噪声假设下，它等价于贝叶斯推断。\n最大熵原理：在某些约束条件下，卷积产生的分布具有最大熵，这解释了为什么多次卷积会趋向正态分布。\n卷积定理：时域卷积与频域乘积的对应关系 让我们来探讨一个卷积的核心性质——卷积定理。这个定理告诉我们：时域中的卷积等价于频域中的乘积。这听起来很神奇，但背后有着深刻的数学原理。\n从数学表达上看，卷积定理可以表示为：\n$$\\mathcal{F}\\{f * g\\} = \\mathcal{F}\\{f\\} \\cdot \\mathcal{F}\\{g\\}$$其中 $\\mathcal{F}$ 表示傅里叶变换，$f * g$ 表示 $f$ 和 $g$ 的卷积。\n为什么会这样？ 这种对应关系的本质可以从卷积的定义出发来理解。回顾卷积的定义：\n$$y(t) = \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau)d\\tau$$当我们对这个式子进行傅里叶变换时，会发生什么？\n傅里叶变换将时域信号分解为不同频率的正弦波的叠加。对于每个频率分量 $\\omega$，卷积的傅里叶变换可以写为：\n$$\\mathcal{F}\\{f * g\\}(\\omega) = \\int_{-\\infty}^{\\infty} \\left( \\int_{-\\infty}^{\\infty} f(\\tau)g(t-\\tau)d\\tau \\right) e^{-j\\omega t} dt$$通过变换顺序和变量替换，可以证明这等价于 $F(\\omega) \\cdot G(\\omega)$，其中 $F(\\omega)$ 和 $G(\\omega)$ 分别是 $f(t)$ 和 $g(t)$ 的傅里叶变换。\n从滑动加权平均的角度理解 还记得我们之前讨论的\u0026quot;滑动加权平均\u0026quot;视角吗？卷积本质上是一种滑动窗口操作，窗口形状保持不变，只是位置在变化。\n在频域中，这种\u0026quot;滑动不变性\u0026quot;表现为什么呢？答案是：频率分量的独立调制。\n每个频率分量都被独立地调整幅度和相位，而不会与其他频率产生\u0026quot;混淆\u0026quot;。这正是乘法的特性！对于频谱 $F(\\omega)$ 中的每个分量，我们只需将其乘以 $G(\\omega)$ 中对应频率的值即可。\n齐次性的体现 这种对应关系也是卷积齐次性的一种体现。我们之前讨论过，卷积中的齐次关系 $\\tau + (t - \\tau) = t$ 体现了时间不变性。在频域中，这种不变性转化为频率分量的独立处理。\n时域中的\u0026quot;滑动不变性\u0026quot;意味着系统对输入的响应只取决于输入与当前时刻的时间差，而不依赖于绝对时间。这种性质在频域中自然对应为各频率分量的独立调制。\n计算效率的提升 这种对偶性不仅具有理论意义，还带来了实际的计算优势。对于长序列的卷积，直接计算的复杂度是 $O(n^2)$，而利用快速傅里叶变换 (FFT)，我们可以将复杂度降低到 $O(n\\log n)$：\n对输入信号进行 FFT 在频域中相乘 进行逆 FFT 得到结果 这种\u0026quot;变换-乘积-逆变换\u0026quot;的策略大大提高了卷积的计算效率，在信号处理、图像处理等领域有着广泛应用。\n总的来说，时域卷积等于频域乘积这一性质，揭示了卷积作为一种数学操作的内在优雅性，它将时域中复杂的积分操作转化为频域中简单的乘法，体现了数学中常见的\u0026quot;复杂问题简单化\u0026quot;的美妙转换。\n总结 卷积本质上是一种时空融合的数学语言，通过\u0026quot;滑动加权平均\u0026quot;将两个函数的全局信息深度融合：在时域体现为系统对历史输入的累积响应，在频域展现为频率分量的优雅调制，在概率论中呈现为随机涨落的光滑收敛，在深度学习中则演化为特征提取的通用范式。这种独特的组合方式——以函数为权重、以积分为纽带、以对称为灵魂——使其成为贯通信号处理、概率论、物理学和人工智能的基础性思维工具，既刻画着自然界的因果律动，也驱动着现代科技的智能演进。(本段总结由 Deepseek 生成)\n","date":"2025-03-21T23:25:38+08:00","permalink":"https://tok1024.com/p/%E5%8D%B7%E7%A7%AF/","title":"卷积"},{"content":"Intro 最近在学习深度学习的基础知识, 对于五花八门的模型深感神奇, 大受震撼, 但是觉得实操能力欠佳, 于是尝试实操手搓一个 Transformer\n训练一个模型有四个步骤: 数据处理 -\u0026gt; 定义模型 -\u0026gt; 定义损失函数 -\u0026gt; 优化, 我们这次也将按照这个步骤进行, 过程参考 b 站视频 Pytorch手搓 Transformer\nTransformer 首先, 什么是 transformer?\n当然不是变形金刚, Transformer 是一个基于 Self-Attention机制的 Seq2seq 的深度学习模型, 能够捕捉上下文的信息和序列数据, 可以并行训练, 现在已经得到广泛的应用, 我们熟悉的 BERT, GPT, Deepseek 都使用了 Transformer 架构, 足以证明其性能的优越性\n我们这次将训练一个非常简单的 transformer, 输入数据是上文, 设定一个生成的文本长度, 然后直接输出下文, 未来可能会把起始和结束标识编码到 embedding 向量中, 但我现在还不会\n数据处理 我们的原始数据是中文的文本文件, 要想存储到计算机中供模型训练, 就需要先把每个字转换为一个 token, 再将 token 经过 Embedding 嵌入为词向量\nToken 化 这一步中, 我们需要创建唯一, 有序的字符集, 然后建立数字即 Token 到字符的映射\n1 2 3 4 5 6 7 8 9 10 11 # 有序的字符集合 chars = sorted(list(set(text))) # 字符到数字的映射 c2i = {c:i for i, c in enumerate(chars)} i2c = {i:c for i, c in enumerate(chars)} # 编码: 字符串 -\u0026gt; 数字列表 # 解码: 数字列表 -\u0026gt; 字符串 encode = lambda x: [c2i[c] for c in x] decode = lambda x: \u0026#34;\u0026#34;.join([i2c[i] for i in x]) 数据分组 训练模型时, 一条一条训练效率过于低下, 我们会选择一次处理一批数据, 这样可以利用 GPU 的并行性, 提高性能, 每一个向量是长度为 block_size 的字符串\n所以一批训练资料是 [batch_size, block_size, embedding_dim] 的三阶张量\nget_batch 对于 batch 的选择, 我们随机在文本中取一段 block\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # TODO: 数据分批 # 1. 划分数据集 # 直接对text进行编码 data = torch.tensor(encode(text), dtype=torch.long) valid_size = int(len(text) * validation_split) valid_data = data[valid_size:] train_data = data[:valid_size] # 2. get_batch函数 # 从数据集中随机取出batch_size个数据 # 输入: split - \u0026#34;valid\u0026#34; or \u0026#34;train\u0026#34; # 输出: (batch_size, block_size)的tensor def get_batch(split): data = valid_data if split == \u0026#34;valid\u0026#34; else train_data idx = torch.randint(0, len(data) - block_size, (batch_size,)) # stack处理一个列表,把一个张量的列表在新的维度上堆叠起来 x = torch.stack([data[i:i+block_size] for i in idx]) y = torch.stack([data[i+1:i+1+block_size] for i in idx]) # x是字符串的列表, y是x的下一个字符的列表 x, y = x.to(device), y.to(device) return x, y get_batch(\u0026#34;train\u0026#34;) 这里用到了 torch.stack, 让我想起来另一个常用的拼接 api torch.cat, 二者有什么区别呢?\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 创建两个示例张量 a = torch.tensor([[1, 2], [3, 4]]) b = torch.tensor([[5, 6], [7, 8]]) # 使用 torch.cat 进行拼接 cat_result = torch.cat((a, b), dim=0) print(\u0026#34;torch.cat 结果：\u0026#34;) print(cat_result) print(\u0026#34;形状：\u0026#34;, cat_result.shape) # 使用 torch.stack 进行拼接 stack_result = torch.stack((a, b), dim=0) print(\u0026#34;\\ntorch.stack 结果：\u0026#34;) print(stack_result) print(\u0026#34;形状：\u0026#34;, stack_result.shape) 输出结果:\n1 2 3 4 5 6 torch.cat 结果： tensor([[1, 2], [3, 4], [5, 6], [7, 8]]) 形状： torch.Size([4, 2]) torch.stack 结果： tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) 形状： torch.Size([2, 2, 2]) 原来 stack 是在新的维度上连接原本的两个张量, 而 cat 是在外层维度拼接两个张量\n词嵌入 我们已经有了字符到数字的映射, 但是现在这个数字没什么含义, 更无法参与运算, 那我们就需要把每个词表示为一个向量, 这就用到了 nn.Embedding 子类, 创建一个 embedding_table, 维护字符集的索引到 Embedding 空间的映射\n一开始时, 嵌入向量随机生成, 然后不断梯度优化, 可能会和真实的语义有一定的相关性\n位置嵌入 我们知道, 在一个文本中, 词语和其在文本中的顺序是有很强的关系的, 这就需要把位置编码词向量, 这里我们同样用 pytorch 提供的 embedding 类进行嵌入, 与原文的正余弦不同, 后续可以进行调整\n最后把词嵌入和位置嵌入的向量相加得到输入向量\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 位置嵌入 position_embedding_table = nn.Embedding(block_size, embedding_dim).to(device) position_ebd = position_embedding_table(torch.arange(block_size).to(device)).unsqueeze(0) def forward(self, x, target=None): B, T = x.shape ve = self.vocab_embedding(x) pe = self.position_embedding(torch.arange(T).to(device)) h = ve + pe # (B, T, E) h = self.blocks(h) h = self.ln_f(h) logits = self.fc(h) 模型构建 基础模型 我们先考虑创建一个简单的傻瓜模型\n把嵌入后的张量输入神经网络, 他会随机输出一个张量, 维度是 [Batch_size, Block_size, Vocabsize], 其中最后一维是归一化的, 代表了下文中每个 Token 的概率, 不过这里是随机生成的\nToken 生成: 从概率分布中抽样出 one-hot 向量, 使用 torch.multinomial\n实现:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # Model class LanguageModel(nn.Module): def __init__(self): super().__init__() # x: (batch_size, block_size) 单位是token # target: (batch_size, block_size) 单位是token # 返回: (batch_size, block_size, vocab_size) logits def forward(self, x, target=None): B, T = x.shape random_tensor = torch.rand(B, T, vocab_size).to(device) logits = random_tensor / random_tensor.sum(dim=-1, keepdim=True) loss = None return logits, loss # 生成 # token_seq: (batch_size, block_size) 上文, 单位是token # max_token: int 最大生成长度 # 返回: (batch_size, max_token) 生成的token序列 def generate(self, token_seq, max_token): for _ in range(max_token): # 取最后block_size个token token_input = token_seq[:, -block_size:] # 计算logits logits, loss = self.forward(token_input) # 取字符串的最后一个字符, 目前还只是网络直接输出的结果 logits = logits[:, -1, :] # softmax,维度是-1,也就是vocabulary的维度 prob = F.softmax(logits, dim=-1) # 采样, 输出是下一个token,形状是(batch_size, 1) next_token = torch.multinomial(prob, 1) # 拼接到token_seq后面, 在时间维度上 token_seq = torch.cat([token_seq, next_token], dim=1) return token_seq[:, -max_token:] 矩阵变换 在神经网络中, 我们不会用嵌入向量来进行计算, 而是把词向量投影到不同的子空间中\nQ: 查询矩阵, 定义了该 Token 如何去访问别的 Token 的信息 K: 键矩阵, 定义了该 Token 给别的矩阵提供哪些信息 V: 值矩阵, 定义了词向量到我们创建的子空间的映射 再看下论文中优美的公式\n$Attention(Q,K,V)=softmax(\\frac{ QK^T} {d_k}​)V$\n几何意义：\n点积：在子空间 Rdk​ 中，计算两个向量的夹角余弦相似度（未缩放时）。 缩放因子 dk​​：防止点积值过大导致梯度消失。 softmax：将相似度转化为概率分布，表示不同位置的重要性权重。 这个公式精准的描述了 Attention 机制, 即用 Q 去查询 K, 对得到的矩阵除以一个缩放因子(防止梯度爆炸), 输入 softmax 得到注意力矩阵, 然后和 V 矩阵相乘后, 得到了 Token 的概率分布\n掩码矩阵 实践中, 注意力矩阵不能全部都有值, 因为一个预测模型不能输入未来的向量, 这样会破坏模型结构\n我们用下三角矩阵来表示\n每一个值预测时, 我们只看上文, 防止答案泄露\n不可训练的矩阵: 三角矩阵, Tril 把右上角取为 0\n单头注意力的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # Heads class Head(nn.Module): def __init__(self, head_size = head_size): super().__init__() self.value = nn.Linear(embedding_dim, head_size, bias=False) self.query = nn.Linear(embedding_dim, head_size, bias=False) self.key = nn.Linear(embedding_dim, head_size, bias=False) # 生成一个不可训练的下三角矩阵 self.register_buffer(\u0026#34;mask\u0026#34;, torch.tril(torch.ones(block_size, block_size))) self.dropout = nn.Dropout(dropout_rate) def forward(self, x): # x: (batch_size, block_size, embedding_dim) # return: (batch_size, block_size, head_size) # 每个head有一个value矩阵, 用于计算attention B, T, C = x.shape Q = self.query(x) K = self.key(x) attention = Q @ K.transpose(-2, -1) * C ** -0.5 attention = attention.masked_fill(self.mask == 0, float(\u0026#39;-inf\u0026#39;)) # 输出的结果是 value向量 * attention V = self.value(x) attention = F.softmax(attention, dim=-1) out = attention @ V # (B, T, head_size) return self.dropout(out) 多头注意力 Attention 机制实际是在模仿人类阅读和写作时的注意力, 那么人都可以三心二意, 机器为什么不行 (\n所以我们把 embedding 分成多份, 分别用多个注意力头去关注整个向量\n实现:\n1 2 3 4 5 6 7 8 9 10 11 12 class MultiHead(nn.Module): def __init__(self): super().__init__() self.heads = nn.ModuleList([Head() for _ in range(head_num)]) self.fc = nn.Linear(head_num * head_size, embedding_dim) self.dropout = nn.Dropout(dropout_rate) def forward(self, x): # x: (batch_size, block_size, embedding_dim) # return: (batch_size, block_size, embedding_dim) out = self.fc(torch.cat([h(x) for h in self.heads], dim=-1)) out = self.dropout(out) return out 残差连接 实践中, 加入残差连接和 Layer Norm 效果会更好\nResidual connection: 也就是把输出的 b 向量加上输入的 a 向量, 一个理解是我们 QKV 矩阵变换实际上计算出的向量, 可以理解为一个词语向量在上下文中的偏移, 要加上原本的向量才更加稳定, 不管怎么样, 他 works, 可以缓解梯度消失问题\nLayer Norm: 区分于 BatchNorm, BN 是对整个 batch 的同一个 dimension 的 feature 进行归一化, LN 是对同一个向量的不同 dimension 归一化\n我们不仅 self-attention 的输出要残差连接和归一化, 输入进 FC 的也要进行残差连接和归一化, 于是直接把这个整体封装成一个 Block\nBlock 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 class Block(nn.Module): def __init__(self): super().__init__() self.ln1 = nn.LayerNorm(embedding_dim) self.ln2 = nn.LayerNorm(embedding_dim) self.sa = MultiHead() self.ff = nn.Sequential( nn.Linear(embedding_dim, hidden_dim), nn.ReLU(), nn.Dropout(dropout_rate), nn.Linear(hidden_dim, embedding_dim), nn.Dropout(dropout_rate) ) def forward(self, x): # x: (batch_size, block_size, embedding_dim) # return: (batch_size, block_size, embedding_dim) out = x + self.sa(self.ln1(x)) out = self.ln2(out + self.ff(out)) return out 值得一提的是, 这里原论文是先输进注意力层, 再 ln, 叫做 post-ln, 有论文说 pre-ln 效果更好, 我也采用了 pre-ln, 不过感觉没啥提升, 可能是我数据太烂了, 没怎么做清洗, 这不是重点\n多级残差网络 为了增加模型的复杂性, 我们会连接多个 block, 形成复杂的网络, 在 pytorch 中也很好实现这一点, 于是模型最终版完成了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 # Model class LanguageModel(nn.Module): def __init__(self): super().__init__() self.vocab_embedding = nn.Embedding(vocab_size, embedding_dim) self.position_embedding = nn.Embedding(block_size, embedding_dim) self.blocks = nn.Sequential(*[Block() for _ in range(num_blocks)]) self.dropout = nn.Dropout(0.2) self.ln_f = nn.LayerNorm(embedding_dim) self.fc = nn.Linear(embedding_dim, vocab_size) # x: (batch_size, block_size) 单位是token # target: (batch_size, block_size) 单位是token # 返回: (batch_size, block_size, vocab_size) logits def forward(self, x, target=None): B, T = x.shape ve = self.vocab_embedding(x) pe = self.position_embedding(torch.arange(T).to(device)) h = ve + pe # (B, T, E) h = self.blocks(h) h = self.ln_f(h) logits = self.fc(h) # 计算loss if target is not None: loss = F.cross_entropy(logits.view(B*T, -1), target.view(-1)) else: loss = None return logits, loss # 生成 # token_seq: (batch_size, block_size) 上文, 单位是token # max_token: int 最大生成长度 # 返回: (batch_size, max_token) 生成的token序列 def generate(self, token_seq, max_token): for _ in range(max_token): # 取最后block_size个token token_input = token_seq[:, -block_size:] # 计算logits logits, loss = self.forward(token_input) # 取字符串的最后一个字符, 目前还只是网络直接输出的结果 logits = logits[:, -1, :] # softmax,维度是-1,也就是vocabulary的维度 prob = F.softmax(logits, dim=-1) # 采样, 输出是下一个token,形状是(batch_size, 1) next_token = torch.multinomial(prob, 1) # 拼接到token_seq后面, 在时间维度上 token_seq = torch.cat([token_seq, next_token], dim=1) return token_seq[:, -max_token:] @torch.no_grad() def estimate(model): splits = [\u0026#34;train\u0026#34;, \u0026#34;valid\u0026#34;] model.eval() out = {} for split in splits: losses = torch.zeros(num_interval) for i in range(num_interval): x, y = get_batch(split) logits, loss = model(x, y) losses[i] = loss.item() out[split] = losses.mean() model.train() return out 复盘 成果 用一些名著训练看看效果吧, 首先调整一下超参数\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # Hyperparameters random_seed = 3221 torch.manual_seed(random_seed) batch_size = 128 block_size = 256 num_blocks = 4 head_num = 12 embedding_dim = 192 validation_split = 0.2 device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34; wrapped_width = 50 hidden_dim = 768 num_epochs = 1000 learning_rate = 5e- weight_decay = 0.06 patience = 100 dropout_rate = 0.1 num_interval = max(num_epochs // 10, 50) # 每5%的epochs或至少每10个epochs验证一次 head_size = embedding_dim // head_num 训练过程耗时 11m 21.8s\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 Epoch 0, Loss: 8.412135124206543 Train Loss: 8.115385055541992 Valid Loss: 8.124889373779297 -------------------------------------------------- Epoch 100, Loss: 5.311680793762207 Train Loss: 5.232851982116699 Valid Loss: 5.768039703369141 -------------------------------------------------- Epoch 200, Loss: 4.538897514343262 Train Loss: 4.509884357452393 Valid Loss: 5.323651313781738 -------------------------------------------------- Epoch 300, Loss: 4.282341480255127 Train Loss: 4.204404354095459 Valid Loss: 5.213500499725342 -------------------------------------------------- Epoch 400, Loss: 4.078436851501465 Train Loss: 4.0135650634765625 Valid Loss: 5.163753032684326 -------------------------------------------------- Epoch 500, Loss: 3.9056577682495117 Train Loss: 3.8425979614257812 Valid Loss: 5.133504867553711 -------------------------------------------------- Epoch 600, Loss: 3.766578435897827 Train Loss: 3.689257860183716 Valid Loss: 5.1122727394104 -------------------------------------------------- Epoch 700, Loss: 3.659522294998169 Train Loss: 3.543461799621582 Valid Loss: 5.107848644256592 -------------------------------------------------- Epoch 800, Loss: 3.543654203414917 Train Loss: 3.4007351398468018 Valid Loss: 5.122027397155762 -------------------------------------------------- Early stopping! 生成点文字看看:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 -------------------------------------------------- 上文: 余。 同时，总的生产规模之扩大，当然增加那种不是直接有赖于个别企业大小的经济。这些经济中最重要的，是由于相关的工业部门的发达而产生的，这些部门互相帮助，也许集 中在同一地方，但无论如何，它们都利用轮船、火车、电报、印刷机等所提供的近代交通便利。像这种来源所产生的各种经济，是任何生产部门都可获得的，而不是完全依靠它自己的 发达：但是，这些经济必然是随着它自己的发达而迅速地和稳步地增大；如果它衰败的话，这些经济在某些方面—— 虽然不是在一切方面——必然是缩小的。 第二节　生产费用应当以一个代表性企业来说明，这 -------------------------------------------------- 真实下文: 个企业能正常地获得属于一定的总生产量的内部经济与外部经济。报酬不变与报酬递增。 当我们研究支配一种商品的供给价格之各种原因时，这些结果具有很大的重要性。我们必 须仔细分析生产一种商品与一定的总生产量有关的正常费用；为了这个目的，我们将要研究在那个总生产量之下一个代表性生产者的费用。一方面，我们不要选择某一刚刚竭力投身营 业的新生产者为代表，他在许多不利的条件下经营，一时不得不满足于很少的利润或没有利润，但他对以下的事实是满意的；他正在建立营业关系，对于建立成功的营业正有头绪；另 一方面，我们也不要采取这样一个企业为代表：由于非常持久的能力和好运气，它已经有了很大的营业和井井有条的大工场，而这些大工 -------------------------------------------------- 生成下文: 些参与备有收入和公司机的关系。雷益似存在深认为，我付不要大多用这一个新加上升的经济信息，它可以先衰退出于他们所作用加就适了此，附高的例外里学习的时就是因素解它。 我的行为使用于学说：无能支付的研究别人地者和银到这家愿意识良好工业，获得将会在因非营销反悔的技能性组织、不同的冒险的一种，看待着较有良好，当地工作更多的陈产也也 就产项目希望财富为是工员。真正越多的和同样的每一种情习这种成为，而是世界上受过的生的心理解它的机会，经纪人就增加了。正式和政府的“后，你想将来看承虚拟时间的梦想 忘镜子。”超过去在太富了贫穷人不变化，还到10美元的“变成虚拟轻松工作”中，而自己也是然寻法律的这一个人所组成了，但现恶 可以看出来效果还是不错的, 虽然没什么语义, 但是标点符号基本都能对上, 看着也像个句子, 嗯, 很满意\n问题分析 可以看到训练过程中出现了明显的过拟合问题, 应该主要是数据不足的问题 训练过程中, 验证集的 loss 计算的很慢, 跟训练的时间都差不多了, 这个后续可以优化一下 超参数和模型没有太多优化, 因为模型训练太慢了, 我也懒得等\u0026hellip; 总结 很好玩的一次实践, 之前一直对 pytorch 里张量的维数有点晕, 实操一次下来就比较清晰了, 对 transformer 的认识也更加清晰了, 非常感谢 b 站 up 黯淡蓝点的居民的视频和 NTU 李宏毅老师的机器学习课程\n","date":"2025-03-15T11:31:55+08:00","permalink":"https://tok1024.com/p/my-transformer/","title":"My Transformer"},{"content":"\nOS 基础 什么是操作系统 操作系统（Operating System，简称 OS）是管理计算机硬件与软件资源的程序，是计算机的管理员 操作系统本质上是一个运行在计算机上的软件程序 ，主要用于管理计算机硬件和软件资源 操作系统存在屏蔽了硬件层的复杂性。 没有什么问题是加一个中间层无法解决的, os 就是硬件和应用程序的中间层 操作系统的 内核(Kernel) 是操作系统的核心部分，它负责系统的内存管理，硬件设备的管理，文件系统的管理以及应用程序的管理。内核是连接应用程序和硬件的桥梁，决定着系统的性能和稳定性。 操作系统有哪些功能 从资源管理的角度来看，操作系统有 6 大功能：\n进程和线程的管理：进程的创建、撤销、阻塞、唤醒，进程间的通信等。 存储管理：内存的分配和管理、外存（磁盘等）的分配和管理等。 文件管理：文件的读、写、创建及删除等。 设备管理：完成设备（输入输出设备和外部存储设备等）的请求或释放，以及设备启动等功能。 网络管理：操作系统负责管理计算机网络的使用 安全管理：用户的身份认证、访问控制、文件加密等，以防止非法用户对系统资源的访问和操作。 用户态和内核态 什么是用户态和内核态\n根据进程对资源的访问权限，我们可以把进程在系统上的运行分为两个级别：\n用户态(User Mode) : 用户态运行的进程可以直接读取用户程序的数据，拥有较低的权限。当应用程序需要执行某些需要特殊权限的操作，例如读写磁盘、网络通信等，就需要向操作系统发起系统调用请求，进入内核态。 内核态(Kernel Mode)：内核态运行的进程几乎可以访问计算机的任何资源包括系统的内存空间、设备、驱动程序等，不受限制，拥有非常高的权限。当操作系统接收到进程的系统调用请求时，就会从用户态切换到内核态，执行相应的系统调用，并将结果返回给进程，最后再从内核态切换回用户态。 为什么要用用户态和内核态? 只有内核态不行吗\n在 CPU 的所有指令中，有一些指令是比较危险的比如内存分配、设置时钟、IO 处理等，叫做特权指令. 我们不希望每个用户都可以执行这些指令 PS: syscall 不是函数, 而是汇编意义上的指令\n如果计算机系统中只有一个内核态，那么所有程序或进程都必须共享系统资源，例如内存、CPU、硬盘等，这将导致系统资源的竞争和冲突，从而影响系统性能和效率。并且，这样也会让系统的安全性降低，毕竟所有程序或进程都具有相同的特权级别和访问权限。 ==用户态和内核态如何切换==\n用户态切换到内核态的 3 种方式：\n系统调用（Trap）：用户态进程 主动 要求切换到内核态的一种方式，主要是为了使用内核态才能做的事情比如读取磁盘资源。 中断（Interrupt）：来自外部的打断, 比如时间片轮转的计时器, 或者 dma 在处理完成文件传输后会给 cpu 一个中断 异常（Exception）：来自程序内部的事先不可知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关程序中，也就转到了内核态，比如缺页异常。 系统调用 什么是系统调用?\n当用户态的程序想要执行 os 提供的功能时, 就需要通过 trap 执行系统调用\n也就是说在我们运行的用户程序中，凡是与系统态级别的资源有关的操作（如文件管理、进程控制、内存管理等)，都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。\n系统调用的过程了解吗\n系统调用的过程可以简单分为以下几个步骤：\n用户态的程序执行到特权指令比如 open, write, malloc 之类的，用户态程序权限不足，因此会中断执行，也就是 Trap 发生中断后，当前 CPU 执行的程序会中断，进入内核态, 跳转到中断处理程序(handler)。内核程序开始执行，也就是开始处理系统调用。 内核处理完成后，主动触发 Trap，这样会再次发生中断，切换回用户态工作。 进程和线程 概念 什么是进程和线程\n进程（Process） 是指计算机中正在运行的一个程序实例。举例：你打开的微信就是一个进程。 线程（Thread） 也被称为轻量级进程，更加轻量。多个线程可以在同一个进程中同时执行，并且共享进程的资源. 线程是程序执行的最小单元 进程与线程的区别\n从上图可以看出：一个进程中可以有多个线程，多个线程共享进程的堆和方法区 (JDK1.8 之后的元空间) 资源，但是每个线程有自己的程序计数器、虚拟机栈 和 本地方法栈。\n线程是进程划分成的更小的运行单位,一个进程在其执行的过程中可以产生多个线程。 进程是 os 提供的隔离和保护的最小单元, 线程是程序执行的最小单元, 是一个个指令流 线程执行开销小，但不利于资源的管理和保护；而进程正相反。 有了进程为什么还需要线程\n进程切换是一个开销很大的操作，线程切换的成本较低。 线程更轻量，一个进程可以创建多个线程。 多个线程可以并发处理不同的任务，更有效地利用了多处理器和多核计算机。而单线程的进程只能在一个时间干一件事，如果在执行过程中遇到阻塞问题比如 IO 阻塞就会挂起直到结果返回。 同一进程内的线程共享内存和文件，因此它们之间相互通信无须调用内核。 为什么要用多线程\n从计算机底层来说： 线程可以比作是轻量级的进程，是程序执行的最小单位,线程间的切换和调度的成本远远小于进程。另外，多核 CPU 时代意味着多个线程可以同时运行，这减少了线程上下文切换的开销。 多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。 减少了单线程程序被阻塞所占用的时间, 提高了 java 进程对系统资源的利用率 当前 cpu 的核心频率由于技术问题难以提升, 所以通过增加 cpu 核心数量来提升性能. 只有多线程才能提高对计算机硬件的利用 线程间同步的方式\n线程同步是两个或多个共享关键资源的线程的并发执行。应该同步线程以避免关键的资源使用冲突。\n互斥锁(Mutex) 读写锁（Read-Write Lock） 信号量(Semaphore) 屏障（Barrier） 事件(Event) PCB 是什么? 包含哪些信息\nPCB（Process Control Block） 即进程控制块，是操作系统中用来管理和跟踪进程的数据结构，每个进程都对应着一个独立的 PCB。你可以将 PCB 视为进程的大脑。\n当操作系统创建一个新进程时，会为该进程分配一个唯一的 PID，并且为该进程创建一个对应的进程控制块。当进程执行时，PCB 中的信息会不断变化，操作系统会根据这些信息来管理和调度进程。\nPCB 主要包含下面几部分的内容：\n进程的描述信息，包括进程的名称、标识符等等； 进程的调度信息，包括进程阻塞原因、进程状态（就绪、运行、阻塞等）、进程优先级（标识进程的重要程度）等等； 进程对资源的需求情况，包括 CPU 时间、内存空间、I/O 设备等等。 进程打开的文件信息，包括文件描述符、文件类型、打开模式等等。 处理机的状态信息（由处理机的各种寄存器中的内容组成的），包括通用寄存器、指令计数器、程序状态字 PSW、用户栈指针 进程调度 进程有哪几种状态\n我们一般把进程大致分为 5 种状态，这一点和线程很像！\n创建状态(new)：进程正在被创建，尚未到就绪状态。 就绪状态(ready)：进程已处于准备运行状态，即进程获得了除了处理器之外的一切所需资源，一旦得到处理器资源(处理器分配的时间片)即可运行。 运行状态(running)：进程正在处理器上运行 阻塞状态(waiting)：又称为等待状态，进程正在等待某一事件而暂停运行 结束状态(terminated)：进程正在从系统中消失。可能是进程正常结束或其他原因中断退出运行。 进程间的通信方式有哪些?\n管道/匿名管道(Pipes) ：用于具有亲缘关系的父子进程间或者兄弟进程之间的通信。 有名管道(Named Pipes) : 匿名管道由于没有名字，只能用于亲缘关系的进程间通信。为了克服这个缺点，提出了有名管道。有名管道严格遵循 先进先出(First In First Out) 。有名管道以磁盘文件的方式存在，可以实现本机任意两个进程通信。 信号(Signal) ：信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生； 消息队列(Message Queuing) ：消息队列是消息的链表,具有特定的格式,存放在内存中并由消息队列标识符标识。管道和消息队列的通信数据都是先进先出的原则。存放于内核中 信号量(Semaphores) ：信号量是一个计数器，用于多进程对共享数据的访问，信号量的意图在于进程间同步。 共享内存(Shared memory) ：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程中对共享内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。 套接字(Sockets) : 此方法主要用于在客户端和服务器之间通过网络进行通信。 进程的调度算法有哪些\n这是一个很重要的知识点！为了确定首先执行哪个进程以及最后执行哪个进程以实现最大 CPU 利用率，计算机科学家已经定义了一些算法，它们是：\n先到先服务调度算法(FCFS，First Come, First Served) : 从就绪队列中选择一个最先进入该队列的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度。 短作业优先的调度算法(SJF，Shortest Job First) : 从就绪队列中选出一个估计运行时间最短的进程为之分配资源，使它立即执行并一直执行到完成或发生某事件而被阻塞放弃占用 CPU 时再重新调度 时间片轮转调度算法（RR，Round-Robin） : 时间片轮转调度是一种最古老，最简单，最公平且使用最广的算法。每个进程被分配一个时间段，称作它的时间片，即该进程允许运行的时间。 多级反馈队列调度算法（MFQ，Multi-level Feedback Queue）：多级反馈队列调度算法既能使高优先级的作业得到响应又能使短作业（进程）迅速完成，因而它是目前被公认的一种较好的进程调度算法，UNIX 操作系统采取的便是这种调度算法。 优先级调度算法（Priority）：为每个流程分配优先级，首先执行具有最高优先级的进程，依此类推。具有相同优先级的进程以 FCFS 方式执行。可以根据内存要求，时间要求或任何其他资源要求来确定优先级。 什么是僵尸进程和孤儿进程\n在 Unix/Linux 系统中，子进程通常是通过 fork()系统调用创建的，该调用会创建一个新的进程，该进程是原有进程的一个副本。子进程和父进程的运行是相互独立的，它们各自拥有自己的 PCB，即使父进程结束了，子进程仍然可以继续运行。\n当一个进程调用 exit()系统调用结束自己的生命时，内核会释放该进程的所有资源，包括打开的文件、占用的内存等，但是该进程对应的 PCB 依然存在于系统中。这些信息只有在父进程调用 wait()或 waitpid()系统调用时才会被释放，以便让父进程得到子进程的状态信息。\n僵尸进程：子进程已经终止，但是其父进程仍在运行，且父进程没有调用 wait()或 waitpid()等系统调用来获取子进程的状态信息，释放子进程占用的资源，导致子进程的 PCB 依然存在于系统中，但无法被进一步使用。这种情况下，子进程被称为“僵尸进程”。避免僵尸进程的产生，父进程需要及时调用 wait()或 waitpid()系统调用来回收子进程。 孤儿进程：一个进程的父进程已经终止或者不存在，但是该进程仍在运行。这种情况下，该进程就是孤儿进程。孤儿进程通常是由于父进程意外终止或未及时调用 wait()或 waitpid()等系统调用来回收子进程导致的。为了避免孤儿进程占用系统资源，操作系统会将孤儿进程的父进程设置为 init 进程（进程号为 1），由 init 进程来回收孤儿进程的资源。 如何查看僵尸进程\ntop 命令\n死锁 什么是死锁\n死锁（Deadlock）描述的是这样一种情况：多个进程/线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于进程/线程被无限期地阻塞，因此程序不可能正常终止。\n举一个操作系统发生死锁的例子\n假设有两个进程 A 和 B，以及两个资源 X 和 Y，它们的分配情况如下：\n进程 占用资源 需求资源 A X Y B Y X 此时，进程 A 占用资源 X 并且请求资源 Y，而进程 B 已经占用了资源 Y 并请求资源 X。两个进程都在等待对方释放资源，无法继续执行，陷入了死锁状态。\n产生死锁的四个必要条件\n互斥：资源必须处于非共享模式，即一次只有一个进程可以使用。如果另一进程申请该资源，那么必须等待直到该资源被释放为止。 占有并等待：一个进程至少应该占有一个资源，并等待另一资源，而该资源被其他进程所占有。 非抢占：资源不能被抢占。只能在持有资源的进程完成任务后，该资源才会被释放。 循环等待：有一组等待进程 {P0, P1,..., Pn}， P0 等待的资源被 P1 占有，P1 等待的资源被 P2 占有，……，Pn-1 等待的资源被 Pn 占有，Pn 等待的资源被 P0 占有。 模拟死锁\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 public class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -\u0026gt; { synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource2\u0026#34;); synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); } } }, \u0026#34;线程 1\u0026#34;).start(); new Thread(() -\u0026gt; { synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource1\u0026#34;); synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); } } }, \u0026#34;线程 2\u0026#34;).start(); } } 死锁的解决方案\n思路: 破坏四个必要条件, 一般考虑破坏 2,4, 占有并等待和非抢占\n死锁的预防\n1、静态分配策略\n静态分配策略可以破坏死锁产生的第二个条件（占有并等待）。所谓静态分配策略，就是指一个进程必须在执行前就申请到它所需要的全部资源，并且知道它所要的资源都得到满足之后才开始执行。进程要么占有所有的资源然后开始执行，要么不占有资源，不会出现占有一些资源等待一些资源的情况。\n静态分配策略逻辑简单，实现也很容易，但这种策略 严重地降低了资源利用率，因为在每个进程所占有的资源中，有些资源是在比较靠后的执行时间里采用的，甚至有些资源是在额外的情况下才使用的，这样就可能造成一个进程占有了一些 几乎不用的资源而使其他需要该资源的进程产生等待 的情况。\n2、层次分配策略\n层次分配策略破坏了产生死锁的第四个条件(循环等待)。在层次分配策略下，所有的资源被分成了多个层次，一个进程得到某一次的一个资源后，它只能再申请较高一层的资源；当一个进程要释放某层的一个资源时，必须先释放所占用的较高层的资源，按这种策略，是不可能出现循环等待链的，因为那样的话，就出现了已经申请了较高层的资源，反而去申请了较低层的资源，不符合层次分配策略，证明略。\n死锁的避免\n上面提到的 破坏 死锁产生的四个必要条件之一就可以成功 预防系统发生死锁 ，但是会导致 低效的进程运行 和 资源使用率 。而死锁的避免相反，它的角度是允许系统中同时存在四个必要条件 ，只要掌握并发进程中与每个进程有关的资源动态申请情况，做出 明智和合理的选择 ，仍然可以避免死锁，因为四大条件仅仅是产生死锁的必要条件。\n我们将系统的状态分为 安全状态 和 不安全状态 ，每当在为申请者分配资源前先测试系统状态，若把系统资源分配给申请者会产生死锁，则拒绝分配，否则接受申请，并为它分配资源。比如银行家算法\n死锁的解除\n当死锁检测程序检测到存在死锁发生时，应设法让其解除，让系统从死锁状态中恢复过来，常用的解除死锁的方法有以下四种：\n立即结束所有进程的执行，重新启动操作系统：这种方法简单，但以前所在的工作全部作废，损失很大。 撤销涉及死锁的所有进程，解除死锁后继续运行：这种方法能彻底打破死锁的循环等待条件，但将付出很大代价，例如有些进程可能已经计算了很长时间，由于被撤销而使产生的部分结果也被消除了，再重新执行时还要再次进行计算。 逐个撤销涉及死锁的进程，回收其资源直至死锁解除。 抢占资源：从涉及死锁的一个或几个进程中抢占资源，把夺得的资源再分配给涉及死锁的进程直至死锁解除。 内存管理 概念 内存管理做了什么?\n操作系统的内存管理非常重要，主要负责下面这些事情：\n内存的分配与回收：对进程所需的内存进行分配和释放，malloc 函数：申请内存，free 函数：释放内存。 地址转换：将程序中的虚拟地址转换成内存中的物理地址。 内存扩充：让每个程序认为自己拥有无限大的内存, 必要时对内存空间阔欧容 内存映射：将一个文件直接映射到进程的进程空间中，这样可以通过内存指针用读写内存的办法直接存取文件内容，速度更快。 内存优化：通过调整内存分配策略和回收算法来优化内存使用效率。 内存隔离：让每个程序认为自己独享整个内存空间, 保证进程之间使用内存互不干扰，避免一些恶意程序通过修改内存来破坏系统的安全性。 什么是内存碎片\n内存碎片是由内存的申请和释放产生的，通常分为下面两种：\n内部内存碎片(Internal Memory Fragmentation，简称为内存碎片)：已经分配给进程使用但未被使用的内存。导致内部内存碎片的主要原因是，当采用固定比例比如 2 的幂次方进行内存分配时，进程所分配的内存可能会比其实际所需要的大。举个例子，一个进程只需要 65 字节的内存，但为其分配了 128（2^7） 大小的内存，那 63 字节的内存就成为了内部内存碎片。 外部内存碎片(External Memory Fragmentation，简称为外部碎片)：由于未分配的连续内存区域太小，以至于不能满足任意进程所需要的内存分配请求，这些小片段且不连续的内存空间被称为外部碎片。也就是说，外部内存碎片指的是那些并未分配给进程但又不能使用的内存。我们后面介绍的分段机制就会导致外部内存碎片。 常见的内存管理方式\n内存管理方式可以简单分为下面两种：\n连续内存管理：为一个用户程序分配一个连续的内存空间，内存利用率一般不高。 非连续内存管理：允许一个程序使用的内存分布在离散或者说不相邻的内存中，相对更加灵活一些。 连续内存管理:\n块式管理 是早期计算机操作系统的一种连续内存管理方式，存在严重的内存碎片问题。\n在 Linux 系统中，连续内存管理采用了 伙伴系统（Buddy System）算法 来实现，基本思想是把内存区域划分为 2 的幂次的 buddy 单元, 然后分配内存时使用二分法, 找到最小的大小合适的单元\n但是这个方法无法解决内部碎片问题, Linux 通过 SLAB 解决了内部碎片问题\n非连续内存管理:\n非连续内存管理存在下面 3 种方式：\n段式管理：以段(一段连续的物理内存)的形式管理/分配物理内存。应用程序的虚拟地址空间被分为大小不等的段，段是有实际意义的 页式管理：把物理内存分为连续等长的物理页，应用程序的虚拟地址空间也被划分为连续等长的虚拟页，是现代操作系统广泛使用的一种内存管理方式。 段页式管理机制：结合了段式管理和页式管理的一种内存管理机制，把物理内存先分成若干段，每个段又继续分成若干大小相等的页 虚拟内存 什么是虚拟内存, 有什么用?\n虚拟内存(Virtual Memory) 是计算机系统内存管理非常重要的一个技术，本质上来说它只是逻辑存在的，是一个假想出来的内存空间，主要作用是作为进程访问主存（物理内存）的桥梁并简化内存管理。\n总的来说, 虚拟内存提供了以下功能\n隔离进程：物理内存通过虚拟地址空间访问，虚拟地址空间与进程一一对应。每个进程都认为自己拥有了整个物理内存，进程之间彼此隔离 提升物理内存利用率：有了虚拟地址空间后，操作系统只需要将进程当前正在使用的部分数据或指令加载入物理内存。 简化内存管理：进程都有一个一致且私有的虚拟地址空间，程序员不用和真正的物理内存打交道，而是借助虚拟地址空间访问物理内存，从而简化了内存管理。 多个进程共享物理内存：多程序共享动态链接库 提高内存使用安全性：控制进程对物理内存的访问，隔离不同进程的访问权限，提高系统的安全性。 提供更大的可使用内存空间：让每个程序认为自己有几乎无限大的可用内存, 必要时交换硬盘中的数据到内存中 什么是虚拟地址和物理地址\n物理地址（Physical Address） 是真正的物理内存中地址，更具体点来说是内存阵列上的单元索引. 每个程序中访问的内存都是虚拟地址（Virtual Address）\n操作系统一般通过 CPU 芯片中的一个重要组件 MMU(Memory Management Unit，内存管理单元) 将虚拟地址转换为物理地址，这个过程被称为 地址翻译/地址转换（Address Translation） 。\n通过 MMU 将虚拟地址转换为物理地址后，再通过总线传到物理内存设备，进而完成相应的物理内存读写请求。\nMMU 将虚拟地址翻译为物理地址的主要机制有两种: 分段机制 和 分页机制\n什么是虚拟地址空间和物理地址空间？\n虚拟地址空间是虚拟地址的集合，是虚拟内存的范围。每一个进程都有一个一致且私有的虚拟地址空间。 物理地址空间是物理地址的集合，是物理内存的范围。 虚拟地址和物理内存地址如何映射\nMMU 将虚拟地址翻译为物理地址的主要机制有 3 种:\n分段机制 分页机制 段页机制 其中，现代操作系统广泛采用分页机制\n分段机制 分段机制（Segmentation） 以段(一段 连续 的物理内存)的形式管理/分配物理内存。应用程序的虚拟地址空间被分为大小不等的段，段是有实际意义的，每个段定义了一组逻辑信息，例如有主程序段 MAIN、子程序段 X、数据段 D 及栈段 S 等\n段表有什么用? 地址翻译过程是怎么样的?\n分段管理通过 段表（Segment Table） 映射虚拟地址和物理地址。\n分段机制下的虚拟地址由两部分组成：\n段表索引：标识着该虚拟地址属于整个虚拟地址空间中的哪一个段。 段内偏移量：相对于该段起始地址的偏移量。 具体的地址翻译过程如下：\nMMU 首先解析得到虚拟地址中的段号； 通过段号去该应用程序的段表中取出对应的段信息（找到对应的段表项）； 从段信息中取出该段的起始地址（物理地址）加上虚拟地址中的段内偏移量得到最终的物理地址。 段表中还存有诸如段长(可用于检查虚拟地址是否超出合法范围)、段类型（该段的类型，例如代码段、数据段等）等信息。\n通过段号一定要找到对应的段表项吗？得到最终的物理地址后对应的物理内存一定存在吗？\n不一定。段表项可能并不存在：\n段表项被删除：软件错误、软件恶意行为等情况可能会导致段表项被删除。 段表项还未创建：如果系统内存不足或者无法分配到连续的物理内存块就会导致段表项无法被创建。 分段机制为什么会导致内存外部碎片\n分段机制容易出现外部内存碎片，即在段与段之间留下碎片空间, 是因为分段机制维护了地址的起始和索引, 这要求物理内存的连续性, 但是进程开启和关闭, 内存重新分配会破坏内存的连续性\n分页机制 分页机制（Paging） 把主存（物理内存）分为连续等长的物理页，应用程序的虚拟地址空间划也被分为连续等长的虚拟页。现代操作系统广泛采用分页机制。\n页表有什么用? 地址翻译过程是怎么样的?\n分页机制下的虚拟地址由两部分组成：\n页号：通过虚拟页号可以从页表中取出对应的物理页号； 页内偏移量：物理页起始地址+页内偏移量=物理内存地址。 可见, 页表维护了虚拟页和物理页的映射关系\n具体的地址翻译过程如下：\nMMU 首先解析得到虚拟地址中的虚拟页号； 通过虚拟页号去该应用程序的页表中取出对应的物理页号（找到对应的页表项）； 用该物理页号对应的物理页起始地址（物理地址）加上虚拟地址中的页内偏移量得到最终的物理地址。 页表中还存有诸如访问标志（标识该页面有没有被访问过）、脏数据标识位等信息。\n通过虚拟页号一定要找到对应的物理页号吗？找到了物理页号得到最终的物理地址后对应的物理页一定存在吗？\n不一定！可能会存在 页缺失 。也就是说，物理内存中没有对应的物理页或者物理内存中有对应的物理页但虚拟页还未和物理页建立映射（对应的页表项不存在）。或者内存不足, 对应的页被交换到磁盘里\n单级页表有什么问题? 为什么需要多级页表?\n常见的页表大小是 4kb, 一个 32 位地址空间需要 $2^{20}$ 个页表条目, 每一个条目是 4byte, 那么每个程序就需要 4mb!\n为了解决这个问题，操作系统引入了 多级页表 ，多级页表对应多个页表，每个页表与前一个页表相关联。32 位系统一般为二级页表，64 位系统一般为四级页表。\n在一个地址空间中大部分空间我们都是没有使用的, 所以一级页面很多条目为空, 这就大大降低了内存\n多级页表属于时间换空间的典型场景，利用增加页表查询的次数减少页表占用的空间。\nTLB 有什么用? 使用 TLB 之后地址翻译流程是怎样的\n目前的问题是: 内存翻译太慢了\n有了多级页表之后, 我们需要多次去访问内存才能查询出物理内存, 但是内存对于 cpu 来说太慢了, 于是引入了 TLB\nTLB 是一块全相联的SRAM 高速缓存, 这意味着其有着极高的随机访存能力. 他被用作页表的 cache.\n有了 TLB 之后, 我们先去 TLB 里查询\n从虚拟地址提取 TLB Tag 和 Index 在 TLB 中查找 如果命中： 直接获取 PPN 与 Page Offset 组合形成物理地址 如果未命中： 需要查询页表 什么是 Page fault?\n页缺失指的是当软件试图访问已映射在虚拟地址空间中，但是目前并未被加载在物理内存中的一个分页时，由 MMU 所发出的中断。\n常见的页缺失有下面这两种：\n硬性页缺失（Hard Page Fault）：虚拟内存空间中的虚拟页尚未分配物理页 软性页缺失（Soft Page Fault）：物理内存中有对应的物理页，但虚拟页还未和物理页建立映射。可能由于被替换或尚未加载. Page Fault 由对应的 handler 处理\n发生上面这两种缺页错误的时候，应用程序访问的是有效的物理内存，只是出现了物理页缺失或者虚拟页和物理页的映射关系未建立的问题。如果应用程序访问的是无效的物理内存的话，还会出现 无效缺页错误（Invalid Page Fault） 。\n常见的页面置换算法有哪些?\n最佳页面置换算法（OPT，Optimal）：优先选择淘汰的页面是以后永不使用的，或者是在最长时间内不再被访问的页面，理论最优, 实际无法实现 先进先出页面置换算法（FIFO，First In First Out） : 最简单的一种页面置换算法，总是淘汰最先进入内存的页面，即选择在内存中驻留时间最久的页面进行淘汰。该算法易于实现和理解，一般只需要通过一个 FIFO 队列即可满足需求。不过，它的性能并不是很好。 最近最久未使用页面置换算法（LRU ，Least Recently Used）：LRU 算法赋予每个页面一个访问字段，用来记录一个页面自上次被访问以来所经历的时间 T，当须淘汰一个页面时，选择现有页面中其 T 值最大的，即最近最久未使用的页面予以淘汰。LRU 算法是根据各页之前的访问情况来实现，因此是易于实现的。 最少使用页面置换算法（LFU，Least Frequently Used） : 和 LRU 算法比较像，不过该置换算法选择的是之前一段时间内使用最少的页面作为淘汰页。 时钟页面置换算法（Clock）：可以认为是一种最近未使用算法，即逐出的页面都是最近没有使用的那个。 为何 Fifo 性能不好?\n经常访问或者需要长期存在的页面会被频繁调入调出：较早调入的页往往是经常被访问或者需要长期存在的页，这些页会被反复调入和调出。 存在 Belady 现象：被置换的页面并不是进程不会访问的，有时就会出现分配的页面数增多但缺页率反而提高的异常现象。 哪种页面置换算法用的多?\nLRU\n分页机制和分段机制有哪些共同点?\n共同点：\n都是非连续内存管理的方式。 都采用了地址映射的方法，将虚拟地址映射到物理地址，以实现对内存的管理和保护。 区别：\n分页机制以页面为单位进行内存管理，而分段机制以段为单位进行内存管理。页的大小是固定的，通常为 2 的幂次方。而段的大小不固定。 页是物理单位，即操作系统将物理内存划分成固定大小的页面，每个页面的大小通常是 2 的幂次方，例如 4KB、8KB 等等。而段则是逻辑单位，是为了满足程序对内存空间的逻辑需求而设计的，通常根据程序中数据和代码的逻辑结构来划分。 分段机制容易出现外部内存碎片，即在段与段之间留下碎片空间(不足以映射给虚拟地址空间中的段)。分页机制解决了外部内存碎片的问题，但仍然可能会出现内部内存碎片。 段页机制 结合了段式管理和页式管理的一种内存管理机制，把物理内存先分成若干段，每个段又继续分成若干大小相等的页。\n在段页式机制下，地址翻译的过程分为两个步骤：\n段式地址映射。 页式地址映射。 Locality 局部性原理是指程序在执行过程中倾向于重复访问最近使用过的数据或指令，或访问附近的数据，这一现象使得缓存和预取技术能够显著提升系统性能。\n时间局部性：由于程序中存在一定的循环或者重复操作，因此会反复访问同一个页或一些特定的页，这就体现了时间局部性的特点。 空间局部性：由于程序中数据和指令的访问通常是具有一定的空间连续性的，因此当访问某个页时，往往会顺带访问其相邻的一些页。 文件系统 基本组成 文件系统是操作系统负责持久化存储数据的子系统, 基本单位是文件.\nLinux 最经典的一句话是：「一切皆文件」，不仅普通的文件和目录，就连块设备、管道、socket 等，也都是统一交给文件系统管理的。\nLinux 文件系统会为每个文件分配两个数据结构：索引节点（index node）和目录项（directory entry），它们主要用来记录文件的元信息和目录层次结构。\n索引节点，也就是 inode，用来记录文件的元信息，比如 inode 编号、文件大小、访问权限、创建时间、修改时间、数据在磁盘的位置等等。索引节点是文件的唯一标识，它们之间一一对应，也同样都会被存储在硬盘中，所以索引节点同样占用磁盘空间。 目录项，也就是 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的层级关联关系。多个目录项关联起来，就会形成目录结构，但它与索引节点不同的是，目录项是由内核维护的一个数据结构，不存放于磁盘，而是缓存在内存。 由于索引节点唯一标识一个文件，而目录项记录着文件的名，所以目录项和索引节点的关系是多对一，也就是说，一个文件可以有多个别字。比如，硬链接的实现就是多个目录项中的索引节点指向同一个文件。\n目录项和目录是一个东西吗?\n目录是个文件，持久化存储在磁盘，而目录项是内核一个数据结构，缓存在内存。\n那文件数据是如何存储在磁盘的呢？\n磁盘读写的最小单位是扇区，扇区的大小只有 512B 大小，很明显，如果每次读写都以这么小为单位，那这读写的效率会非常低。\n所以，文件系统把多个扇区组成了一个逻辑块，每次读写的最小单位就是逻辑块（数据块），Linux 中的逻辑块大小为 4KB，也就是一次性读写 8 个扇区，这将大大提高了磁盘的读写的效率。\n另外, 磁盘进行格式化的时候，会被分成三个存储区域，分别是超级块、索引节点区和数据块区。\n超级块，用来存储文件系统的详细信息，比如块个数、块大小、空闲块等等。 索引节点区，用来存储索引节点； 数据块区，用来存储文件或目录数据； 我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使用的时候，才将其加载进内存，它们加载进内存的时机是不同的：\n超级块：当文件系统挂载时进入内存 索引节点区：当文件被访问时进入内存 基本概念 文件系统主要做了什么?\n存储管理：将文件数据存储到物理存储介质中，并且管理空间分配，以确保每个文件都有足够的空间存储，并避免文件之间发生冲突。 文件管理：文件的创建、删除、移动、重命名、压缩、加密、共享等等。 目录管理：目录的创建、删除、移动、重命名等等。 文件访问控制：管理不同用户或进程对文件的访问权限，以确保用户只能访问其被授权访问的文件，以保证文件的安全性和保密性。 硬链接和软链接有什么区别\n在 Linux/类 Unix 系统上，文件链接（File Link）是一种特殊的文件类型，可以在文件系统中指向另一个文件。常见的文件链接类型有两种：\n1、硬链接（Hard Link）\n硬链接通过 inode 节点号建立连接，硬链接和源文件的 inode 节点号相同，两者对文件系统来说是完全平等的（可以看作是互为硬链接，源头是同一份文件），删除其中任何一个对另外一个没有影响，可以通过给文件设置硬链接文件来防止重要文件被误删。 只有删除了源文件和所有对应的硬链接文件，该文件才会被真正删除。 硬链接具有一些限制，不能对目录以及不存在的文件创建硬链接，并且，硬链接也不能跨越文件系统。 ln 命令用于创建硬链接。 2、软链接（Symbolic Link 或 Symlink）\n软链接和源文件的 inode 节点号不同，而是指向一个文件路径。 源文件删除后，软链接依然存在，但是指向的是一个无效的文件路径。 软连接类似于 Windows 系统中的快捷方式。 不同于硬链接，可以对目录或者不存在的文件创建软链接，并且，软链接可以跨越文件系统。 ln -s 命令用于创建软链接。 硬链接为什么不能跨文件系统?\n我们之前提到过，硬链接是通过 inode 节点号建立连接的，而硬链接和源文件共享相同的 inode 节点号。\n然而，每个文件系统都有自己的独立 inode 表，且每个 inode 表只维护该文件系统内的 inode。如果在不同的文件系统之间创建硬链接，可能会导致 inode 节点号冲突的问题，即目标文件的 inode 节点号已经在该文件系统中被使用。\n提高文件系统性能的方式有哪些?\n优化硬件：使用高速硬件设备（如 SSD、NVMe）替代传统的机械硬盘，使用 RAID（Redundant Array of Inexpensive Disks）等技术提高磁盘性能。 选择合适的文件系统选型：不同的文件系统具有不同的特性，对于不同的应用场景选择合适的文件系统可以提高系统性能。 运用缓存：访问磁盘的效率比较低，可以运用缓存来减少磁盘的访问次数。不过，需要注意缓存命中率，缓存命中率过低的话，效果太差。 避免磁盘过度使用：注意磁盘的使用率，避免将磁盘用满，尽量留一些剩余空间，以免对文件系统的性能产生负面影响。 对磁盘进行合理的分区：合理的磁盘分区方案，能够使文件系统在不同的区域存储文件，从而减少文件碎片，提高文件读写性能。 常见的磁盘调度算法有哪些?\n磁盘调度算法是操作系统中对磁盘访问请求进行排序和调度的算法，其目的是提高磁盘的访问效率。\n先来先服务算法（First-Come First-Served，FCFS）：按照请求到达磁盘调度器的顺序进行处理，先到达的请求的先被服务。FCFS 算法实现起来比较简单，不存在算法开销。不过，由于没有考虑磁头移动的路径和方向，平均寻道时间较长。同时，该算法容易出现饥饿问题，即一些后到的磁盘请求可能需要等待很长时间才能得到服务。 最短寻道时间优先算法（Shortest Seek Time First，SSTF）：也被称为最佳服务优先（Shortest Service Time First，SSTF）算法，优先选择距离当前磁头位置最近的请求进行服务。SSTF 算法能够最小化磁头的寻道时间，但容易出现饥饿问题 扫描算法（SCAN）：也被称为电梯（Elevator）算法，基本思想和电梯非常类似。磁头沿着一个方向扫描磁盘，如果经过的磁道有请求就处理，直到到达磁盘的边界，然后改变移动方向，依此往复。SCAN 算法能够保证所有的请求得到服务，解决了饥饿问题。但是，如果磁头从一个方向刚扫描完，请求才到的话。这个请求就需要等到磁头从相反方向过来之后才能得到处理。 循环扫描算法（Circular Scan，C-SCAN）：SCAN 算法的变体，只在磁盘的一侧进行扫描，并且只按照一个方向扫描，直到到达磁盘边界，然后回到磁盘起点，重新开始循环。 边扫描边观察算法（LOOK）：SCAN 算法中磁头到了磁盘的边界才改变移动方向，这样可能会做很多无用功，因为磁头移动方向上可能已经没有请求需要处理了。LOOK 算法对 SCAN 算法进行了改进，如果磁头移动方向上已经没有别的请求，就可以立即改变磁头移动方向，依此往复。也就是边扫描边观察指定方向上还有无请求，因此叫 LOOK。 均衡循环扫描算法（C-LOOK）：C-SCAN 只有到达磁盘边界时才能改变磁头移动方向，并且磁头返回时也需要返回到磁盘起点，这样可能会做很多无用功。C-LOOK 算法对 C-SCAN 算法进行了改进，如果磁头移动的方向上已经没有磁道访问请求了，就可以立即让磁头返回，并且磁头只需要返回到有磁道访问请求的位置即可。 ","date":"2025-03-04T10:46:33+08:00","permalink":"https://tok1024.com/p/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/","title":"操作系统"},{"content":"我的第一个文章\n","date":"2025-03-03T15:41:03+08:00","permalink":"https://tok1024.com/p/my-first-page/","title":"My First Page"}]